{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d2ae866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aayam/micromamba/envs/mnist/lib/python3.11/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  entry = pickle.load(f, encoding=\"latin1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.9894737..2.100835].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std  = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "train_data= datasets.CIFAR10(root=\"./data\", train=True, transform=train_tf, download=True)\n",
    "test_data= datasets.CIFAR10(root=\"./data\", train=False, transform=test_tf, download=True)\n",
    "\n",
    "image, label= train_data[1]\n",
    "\n",
    "plt.imshow(image.permute(1,2,0))\n",
    "plt.show()\n",
    "\n",
    "classes = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1012ad75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625\n",
      "157\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set= random_split(train_data, [40000,10000])\n",
    "\n",
    "train_loader= DataLoader(train_set, batch_size= 64, shuffle=True)\n",
    "val_loader= DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "test_loader= DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7306379",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e89bf0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1= nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1= nn.BatchNorm2d(32)\n",
    "        self.relu1= nn.ReLU()\n",
    "\n",
    "        self.conv2= nn.Conv2d(32, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2= nn.BatchNorm2d(128)\n",
    "        self.relu2= nn.ReLU()\n",
    "\n",
    "        self.conv3= nn.Conv2d(128,256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3= nn.BatchNorm2d(256)\n",
    "        self.relu3= nn.ReLU()\n",
    "\n",
    "        self.conv4= nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn4= nn.BatchNorm2d(512)\n",
    "        self.relu4= nn.ReLU()\n",
    "\n",
    "        self.gap= nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten= nn.Flatten()\n",
    "        self.drop= nn.Dropout(0.3)\n",
    "        self.fc1= nn.Linear(512,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))\n",
    "        x = self.relu2(self.bn2(self.conv2(x)))\n",
    "        x = self.relu3(self.bn3(self.conv3(x)))\n",
    "        x = self.relu4(self.bn4(self.conv4(x)))\n",
    "\n",
    "        x = self.gap(x)        \n",
    "        x = self.flatten(x)  \n",
    "        x=  self.drop(x)\n",
    "        x = self.fc1(x)         \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d69ffd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/30] | batch [100/625] | loss : 1.6448547840118408 \n",
      "epoch [1/30] | batch [200/625] | loss : 1.2670232057571411 \n",
      "epoch [1/30] | batch [300/625] | loss : 1.3503854274749756 \n",
      "epoch [1/30] | batch [400/625] | loss : 1.20161771774292 \n",
      "epoch [1/30] | batch [500/625] | loss : 1.0560157299041748 \n",
      "epoch [1/30] | batch [600/625] | loss : 1.1494196653366089 \n",
      "epoch 1/30 avg train loss: 1.4967782550811768 \n",
      "epoch 1/30 avg val loss: 1.3934705166300392 \n",
      "epoch [2/30] | batch [100/625] | loss : 1.3594192266464233 \n",
      "epoch [2/30] | batch [200/625] | loss : 1.1715816259384155 \n",
      "epoch [2/30] | batch [300/625] | loss : 1.1051501035690308 \n",
      "epoch [2/30] | batch [400/625] | loss : 1.094744086265564 \n",
      "epoch [2/30] | batch [500/625] | loss : 0.94996577501297 \n",
      "epoch [2/30] | batch [600/625] | loss : 1.1734271049499512 \n",
      "epoch 2/30 avg train loss: 1.1808737441062926 \n",
      "epoch 2/30 avg val loss: 1.1160989525211844 \n",
      "epoch [3/30] | batch [100/625] | loss : 1.1206477880477905 \n",
      "epoch [3/30] | batch [200/625] | loss : 1.1683008670806885 \n",
      "epoch [3/30] | batch [300/625] | loss : 0.9463711977005005 \n",
      "epoch [3/30] | batch [400/625] | loss : 1.0310512781143188 \n",
      "epoch [3/30] | batch [500/625] | loss : 0.8710536956787109 \n",
      "epoch [3/30] | batch [600/625] | loss : 1.1511586904525757 \n",
      "epoch 3/30 avg train loss: 1.0489448431015014 \n",
      "epoch 3/30 avg val loss: 1.0839246128015458 \n",
      "epoch [4/30] | batch [100/625] | loss : 0.9386464953422546 \n",
      "epoch [4/30] | batch [200/625] | loss : 1.0378718376159668 \n",
      "epoch [4/30] | batch [300/625] | loss : 0.9976736903190613 \n",
      "epoch [4/30] | batch [400/625] | loss : 0.9730331301689148 \n",
      "epoch [4/30] | batch [500/625] | loss : 1.5973801612854004 \n",
      "epoch [4/30] | batch [600/625] | loss : 0.7683495283126831 \n",
      "epoch 4/30 avg train loss: 0.9541078503608703 \n",
      "epoch 4/30 avg val loss: 1.0633574268620485 \n",
      "epoch [5/30] | batch [100/625] | loss : 0.9491851329803467 \n",
      "epoch [5/30] | batch [200/625] | loss : 0.7073399424552917 \n",
      "epoch [5/30] | batch [300/625] | loss : 0.727382242679596 \n",
      "epoch [5/30] | batch [400/625] | loss : 0.7420014142990112 \n",
      "epoch [5/30] | batch [500/625] | loss : 0.7342925667762756 \n",
      "epoch [5/30] | batch [600/625] | loss : 0.7447740435600281 \n",
      "epoch 5/30 avg train loss: 0.8811784141540527 \n",
      "epoch 5/30 avg val loss: 0.9025614652664039 \n",
      "epoch [6/30] | batch [100/625] | loss : 0.8459512591362 \n",
      "epoch [6/30] | batch [200/625] | loss : 0.8842601776123047 \n",
      "epoch [6/30] | batch [300/625] | loss : 0.831719696521759 \n",
      "epoch [6/30] | batch [400/625] | loss : 0.9916749596595764 \n",
      "epoch [6/30] | batch [500/625] | loss : 0.5444745421409607 \n",
      "epoch [6/30] | batch [600/625] | loss : 0.7195841073989868 \n",
      "epoch 6/30 avg train loss: 0.8261479598522187 \n",
      "epoch 6/30 avg val loss: 0.959299680154035 \n",
      "epoch [7/30] | batch [100/625] | loss : 0.6352341175079346 \n",
      "epoch [7/30] | batch [200/625] | loss : 0.826008677482605 \n",
      "epoch [7/30] | batch [300/625] | loss : 0.6095824837684631 \n",
      "epoch [7/30] | batch [400/625] | loss : 0.7060808539390564 \n",
      "epoch [7/30] | batch [500/625] | loss : 0.9432118535041809 \n",
      "epoch [7/30] | batch [600/625] | loss : 0.6842684745788574 \n",
      "epoch 7/30 avg train loss: 0.7727534299850464 \n",
      "epoch 7/30 avg val loss: 0.7991578034154928 \n",
      "epoch [8/30] | batch [100/625] | loss : 0.6083325743675232 \n",
      "epoch [8/30] | batch [200/625] | loss : 0.7964617609977722 \n",
      "epoch [8/30] | batch [300/625] | loss : 0.7274098992347717 \n",
      "epoch [8/30] | batch [400/625] | loss : 0.8738189339637756 \n",
      "epoch [8/30] | batch [500/625] | loss : 0.6553164720535278 \n",
      "epoch [8/30] | batch [600/625] | loss : 0.5025278925895691 \n",
      "epoch 8/30 avg train loss: 0.7304187695980072 \n",
      "epoch 8/30 avg val loss: 0.7828531240581706 \n",
      "epoch [9/30] | batch [100/625] | loss : 0.547053337097168 \n",
      "epoch [9/30] | batch [200/625] | loss : 0.5697052478790283 \n",
      "epoch [9/30] | batch [300/625] | loss : 0.769493818283081 \n",
      "epoch [9/30] | batch [400/625] | loss : 0.6271967887878418 \n",
      "epoch [9/30] | batch [500/625] | loss : 0.712864339351654 \n",
      "epoch [9/30] | batch [600/625] | loss : 0.7984668016433716 \n",
      "epoch 9/30 avg train loss: 0.6953842700004578 \n",
      "epoch 9/30 avg val loss: 0.819365229955904 \n",
      "epoch [10/30] | batch [100/625] | loss : 0.6704738736152649 \n",
      "epoch [10/30] | batch [200/625] | loss : 0.9032853841781616 \n",
      "epoch [10/30] | batch [300/625] | loss : 0.7853739261627197 \n",
      "epoch [10/30] | batch [400/625] | loss : 0.607155978679657 \n",
      "epoch [10/30] | batch [500/625] | loss : 0.7887553572654724 \n",
      "epoch [10/30] | batch [600/625] | loss : 0.6657754182815552 \n",
      "epoch 10/30 avg train loss: 0.6661184145927429 \n",
      "epoch 10/30 avg val loss: 0.7348942403580733 \n",
      "epoch [11/30] | batch [100/625] | loss : 0.5413621664047241 \n",
      "epoch [11/30] | batch [200/625] | loss : 0.929777979850769 \n",
      "epoch [11/30] | batch [300/625] | loss : 0.8503298163414001 \n",
      "epoch [11/30] | batch [400/625] | loss : 0.5410969257354736 \n",
      "epoch [11/30] | batch [500/625] | loss : 0.5995200276374817 \n",
      "epoch [11/30] | batch [600/625] | loss : 0.7546108365058899 \n",
      "epoch 11/30 avg train loss: 0.639292954826355 \n",
      "epoch 11/30 avg val loss: 0.684784699966953 \n",
      "epoch [12/30] | batch [100/625] | loss : 0.7870969176292419 \n",
      "epoch [12/30] | batch [200/625] | loss : 0.6192194223403931 \n",
      "epoch [12/30] | batch [300/625] | loss : 0.580723762512207 \n",
      "epoch [12/30] | batch [400/625] | loss : 0.5392798781394958 \n",
      "epoch [12/30] | batch [500/625] | loss : 0.7221269607543945 \n",
      "epoch [12/30] | batch [600/625] | loss : 0.49046942591667175 \n",
      "epoch 12/30 avg train loss: 0.6099421207904816 \n",
      "epoch 12/30 avg val loss: 0.6955813296661255 \n",
      "epoch [13/30] | batch [100/625] | loss : 0.5925517678260803 \n",
      "epoch [13/30] | batch [200/625] | loss : 0.5102108120918274 \n",
      "epoch [13/30] | batch [300/625] | loss : 0.5940806865692139 \n",
      "epoch [13/30] | batch [400/625] | loss : 0.5522394776344299 \n",
      "epoch [13/30] | batch [500/625] | loss : 0.5040754675865173 \n",
      "epoch [13/30] | batch [600/625] | loss : 0.5029487609863281 \n",
      "epoch 13/30 avg train loss: 0.5865333737850189 \n",
      "epoch 13/30 avg val loss: 0.7128346478863127 \n",
      "epoch [14/30] | batch [100/625] | loss : 0.5559366345405579 \n",
      "epoch [14/30] | batch [200/625] | loss : 0.50313800573349 \n",
      "epoch [14/30] | batch [300/625] | loss : 0.5419358611106873 \n",
      "epoch [14/30] | batch [400/625] | loss : 0.33178436756134033 \n",
      "epoch [14/30] | batch [500/625] | loss : 0.5986826419830322 \n",
      "epoch [14/30] | batch [600/625] | loss : 0.44661569595336914 \n",
      "epoch 14/30 avg train loss: 0.5644484286785125 \n",
      "epoch 14/30 avg val loss: 0.6809605883944566 \n",
      "epoch [15/30] | batch [100/625] | loss : 0.4461534917354584 \n",
      "epoch [15/30] | batch [200/625] | loss : 0.5522680878639221 \n",
      "epoch [15/30] | batch [300/625] | loss : 0.4221287965774536 \n",
      "epoch [15/30] | batch [400/625] | loss : 0.45261573791503906 \n",
      "epoch [15/30] | batch [500/625] | loss : 0.4173380136489868 \n",
      "epoch [15/30] | batch [600/625] | loss : 0.5827836394309998 \n",
      "epoch 15/30 avg train loss: 0.5483765180587769 \n",
      "epoch 15/30 avg val loss: 0.6247956006769921 \n",
      "epoch [16/30] | batch [100/625] | loss : 0.651822030544281 \n",
      "epoch [16/30] | batch [200/625] | loss : 0.36672189831733704 \n",
      "epoch [16/30] | batch [300/625] | loss : 0.5582593679428101 \n",
      "epoch [16/30] | batch [400/625] | loss : 0.4776984453201294 \n",
      "epoch [16/30] | batch [500/625] | loss : 0.39673009514808655 \n",
      "epoch [16/30] | batch [600/625] | loss : 0.40471571683883667 \n",
      "epoch 16/30 avg train loss: 0.5290181628227234 \n",
      "epoch 16/30 avg val loss: 0.6428558272161301 \n",
      "epoch [17/30] | batch [100/625] | loss : 0.5641065835952759 \n",
      "epoch [17/30] | batch [200/625] | loss : 0.41770946979522705 \n",
      "epoch [17/30] | batch [300/625] | loss : 0.41610729694366455 \n",
      "epoch [17/30] | batch [400/625] | loss : 0.43703609704971313 \n",
      "epoch [17/30] | batch [500/625] | loss : 0.48355191946029663 \n",
      "epoch [17/30] | batch [600/625] | loss : 0.6728529334068298 \n",
      "epoch 17/30 avg train loss: 0.5071822027206421 \n",
      "epoch 17/30 avg val loss: 0.6068912909668722 \n",
      "epoch [18/30] | batch [100/625] | loss : 0.5088799595832825 \n",
      "epoch [18/30] | batch [200/625] | loss : 0.5214716196060181 \n",
      "epoch [18/30] | batch [300/625] | loss : 0.48674702644348145 \n",
      "epoch [18/30] | batch [400/625] | loss : 0.6185725331306458 \n",
      "epoch [18/30] | batch [500/625] | loss : 0.45867863297462463 \n",
      "epoch [18/30] | batch [600/625] | loss : 0.5034661889076233 \n",
      "epoch 18/30 avg train loss: 0.49525567235946655 \n",
      "epoch 18/30 avg val loss: 0.5702877976712147 \n",
      "epoch [19/30] | batch [100/625] | loss : 0.4957241117954254 \n",
      "epoch [19/30] | batch [200/625] | loss : 0.49832192063331604 \n",
      "epoch [19/30] | batch [300/625] | loss : 0.5818842649459839 \n",
      "epoch [19/30] | batch [400/625] | loss : 0.5014554858207703 \n",
      "epoch [19/30] | batch [500/625] | loss : 0.4156797230243683 \n",
      "epoch [19/30] | batch [600/625] | loss : 0.5397058129310608 \n",
      "epoch 19/30 avg train loss: 0.4809096057653427 \n",
      "epoch 19/30 avg val loss: 0.5575546067041955 \n",
      "epoch [20/30] | batch [100/625] | loss : 0.38417211174964905 \n",
      "epoch [20/30] | batch [200/625] | loss : 0.5188498497009277 \n",
      "epoch [20/30] | batch [300/625] | loss : 0.5346003174781799 \n",
      "epoch [20/30] | batch [400/625] | loss : 0.3408205807209015 \n",
      "epoch [20/30] | batch [500/625] | loss : 0.456762433052063 \n",
      "epoch [20/30] | batch [600/625] | loss : 0.45299243927001953 \n",
      "epoch 20/30 avg train loss: 0.46451781363487243 \n",
      "epoch 20/30 avg val loss: 0.5179448719996556 \n",
      "epoch [21/30] | batch [100/625] | loss : 0.3092019259929657 \n",
      "epoch [21/30] | batch [200/625] | loss : 0.3818480670452118 \n",
      "epoch [21/30] | batch [300/625] | loss : 0.6709120273590088 \n",
      "epoch [21/30] | batch [400/625] | loss : 0.524205207824707 \n",
      "epoch [21/30] | batch [500/625] | loss : 0.3910616636276245 \n",
      "epoch [21/30] | batch [600/625] | loss : 0.6349977254867554 \n",
      "epoch 21/30 avg train loss: 0.4491024022102356 \n",
      "epoch 21/30 avg val loss: 0.5134763138689054 \n",
      "epoch [22/30] | batch [100/625] | loss : 0.4343912899494171 \n",
      "epoch [22/30] | batch [200/625] | loss : 0.3823971748352051 \n",
      "epoch [22/30] | batch [300/625] | loss : 0.46635177731513977 \n",
      "epoch [22/30] | batch [400/625] | loss : 0.8209983706474304 \n",
      "epoch [22/30] | batch [500/625] | loss : 0.21832267940044403 \n",
      "epoch [22/30] | batch [600/625] | loss : 0.46712690591812134 \n",
      "epoch 22/30 avg train loss: 0.43867275002002715 \n",
      "epoch 22/30 avg val loss: 0.5556767207042427 \n",
      "epoch [23/30] | batch [100/625] | loss : 0.556729793548584 \n",
      "epoch [23/30] | batch [200/625] | loss : 0.609406590461731 \n",
      "epoch [23/30] | batch [300/625] | loss : 0.4067946970462799 \n",
      "epoch [23/30] | batch [400/625] | loss : 0.5430727005004883 \n",
      "epoch [23/30] | batch [500/625] | loss : 0.3396405577659607 \n",
      "epoch [23/30] | batch [600/625] | loss : 0.5392599105834961 \n",
      "epoch 23/30 avg train loss: 0.42181531386375426 \n",
      "epoch 23/30 avg val loss: 0.5158793762990623 \n",
      "epoch [24/30] | batch [100/625] | loss : 0.37297096848487854 \n",
      "epoch [24/30] | batch [200/625] | loss : 0.3257259130477905 \n",
      "epoch [24/30] | batch [300/625] | loss : 0.5399516820907593 \n",
      "epoch [24/30] | batch [400/625] | loss : 0.49707627296447754 \n",
      "epoch [24/30] | batch [500/625] | loss : 0.5311474204063416 \n",
      "epoch [24/30] | batch [600/625] | loss : 0.5533291101455688 \n",
      "epoch 24/30 avg train loss: 0.4136466134786606 \n",
      "epoch 24/30 avg val loss: 0.5464506513753514 \n",
      "epoch [25/30] | batch [100/625] | loss : 0.27786317467689514 \n",
      "epoch [25/30] | batch [200/625] | loss : 0.36156076192855835 \n",
      "epoch [25/30] | batch [300/625] | loss : 0.4457281231880188 \n",
      "epoch [25/30] | batch [400/625] | loss : 0.3124368190765381 \n",
      "epoch [25/30] | batch [500/625] | loss : 0.3491106629371643 \n",
      "epoch [25/30] | batch [600/625] | loss : 0.3808634281158447 \n",
      "epoch 25/30 avg train loss: 0.4034226335287094 \n",
      "epoch 25/30 avg val loss: 0.543081642905618 \n",
      "epoch [26/30] | batch [100/625] | loss : 0.24537593126296997 \n",
      "epoch [26/30] | batch [200/625] | loss : 0.30387741327285767 \n",
      "epoch [26/30] | batch [300/625] | loss : 0.2861189544200897 \n",
      "epoch [26/30] | batch [400/625] | loss : 0.4644641876220703 \n",
      "epoch [26/30] | batch [500/625] | loss : 0.37886980175971985 \n",
      "epoch [26/30] | batch [600/625] | loss : 0.4142235815525055 \n",
      "epoch 26/30 avg train loss: 0.3945389281272888 \n",
      "epoch 26/30 avg val loss: 0.48974404536235105 \n",
      "epoch [27/30] | batch [100/625] | loss : 0.28825271129608154 \n",
      "epoch [27/30] | batch [200/625] | loss : 0.3549463152885437 \n",
      "epoch [27/30] | batch [300/625] | loss : 0.26667723059654236 \n",
      "epoch [27/30] | batch [400/625] | loss : 0.3096545934677124 \n",
      "epoch [27/30] | batch [500/625] | loss : 0.49221745133399963 \n",
      "epoch [27/30] | batch [600/625] | loss : 0.39846330881118774 \n",
      "epoch 27/30 avg train loss: 0.37846516335010527 \n",
      "epoch 27/30 avg val loss: 0.49361271843029436 \n",
      "epoch [28/30] | batch [100/625] | loss : 0.4192509949207306 \n",
      "epoch [28/30] | batch [200/625] | loss : 0.5169395208358765 \n",
      "epoch [28/30] | batch [300/625] | loss : 0.3358050286769867 \n",
      "epoch [28/30] | batch [400/625] | loss : 0.3735690712928772 \n",
      "epoch [28/30] | batch [500/625] | loss : 0.3885650634765625 \n",
      "epoch [28/30] | batch [600/625] | loss : 0.5081174373626709 \n",
      "epoch 28/30 avg train loss: 0.3783291045665741 \n",
      "epoch 28/30 avg val loss: 0.5926676582378946 \n",
      "epoch [29/30] | batch [100/625] | loss : 0.3269016146659851 \n",
      "epoch [29/30] | batch [200/625] | loss : 0.4859810173511505 \n",
      "epoch [29/30] | batch [300/625] | loss : 0.3190731406211853 \n",
      "epoch [29/30] | batch [400/625] | loss : 0.2851179242134094 \n",
      "epoch [29/30] | batch [500/625] | loss : 0.7301905155181885 \n",
      "epoch [29/30] | batch [600/625] | loss : 0.30143722891807556 \n",
      "epoch 29/30 avg train loss: 0.36810573917627337 \n",
      "epoch 29/30 avg val loss: 0.5023024972457035 \n",
      "epoch [30/30] | batch [100/625] | loss : 0.39933452010154724 \n",
      "epoch [30/30] | batch [200/625] | loss : 0.422409325838089 \n",
      "epoch [30/30] | batch [300/625] | loss : 0.308294415473938 \n",
      "epoch [30/30] | batch [400/625] | loss : 0.298035204410553 \n",
      "epoch [30/30] | batch [500/625] | loss : 0.5616633892059326 \n",
      "epoch [30/30] | batch [600/625] | loss : 0.5987359881401062 \n",
      "epoch 30/30 avg train loss: 0.3522712380051613 \n",
      "epoch 30/30 avg val loss: 0.4805220084585202 \n"
     ]
    }
   ],
   "source": [
    "model= ObjectClassifier().to(device)\n",
    "\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer= torch.optim.AdamW(model.parameters(), lr= 0.001, weight_decay=1e-4)\n",
    "scheduler= torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=6)\n",
    "\n",
    "num_epochs=30\n",
    "\n",
    "best_loss= float(\"inf\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    running_loss=0\n",
    "\n",
    "    for batch_idx, (images,labels) in enumerate(train_loader):\n",
    "\n",
    "        images= images.to(device)\n",
    "        labels= labels.to(device)\n",
    "\n",
    "        output= model(images)\n",
    "        loss= criterion(output, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (batch_idx+1)%100==0:\n",
    "\n",
    "            print(f\"epoch [{epoch+1}/{num_epochs}] | batch [{batch_idx+1}/{len(train_loader)}] | loss : {loss.item()} \")\n",
    "\n",
    "    average_train_loss= running_loss/len(train_loader)\n",
    "    print(f\"epoch {epoch+1}/{num_epochs} avg train loss: {average_train_loss} \")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_val_loss=0\n",
    "\n",
    "        for image,label in val_loader:\n",
    "            image= image.to(device)\n",
    "            label= label.to(device)\n",
    "\n",
    "            output= model(image)\n",
    "            loss= criterion(output,label)\n",
    "\n",
    "            running_val_loss+= loss.item()\n",
    "\n",
    "        avg_val_loss= running_val_loss/len(val_loader)\n",
    "        print(f\"epoch {epoch+1}/{num_epochs} avg val loss: {avg_val_loss} \")\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss= avg_val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "228101d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17098/1644180327.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.68\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "total=0\n",
    "correct=0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images,labels in test_loader:\n",
    "        images= images.to(device)\n",
    "        labels= labels.to(device)\n",
    "\n",
    "        output=model(images)\n",
    "        _,predicted= torch.max(output,1)\n",
    "\n",
    "        total+= images.size(0)\n",
    "        correct+= (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy= 100 * correct/total\n",
    "    print(accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
