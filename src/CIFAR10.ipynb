{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d2ae866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aayam/micromamba/envs/mnist/lib/python3.11/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  entry = pickle.load(f, encoding=\"latin1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.9894737..2.100835].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std  = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "train_data= datasets.CIFAR10(root=\"./data\", train=True, transform=train_tf, download=True)\n",
    "test_data= datasets.CIFAR10(root=\"./data\", train=False, transform=test_tf, download=True)\n",
    "\n",
    "image, label= train_data[1]\n",
    "\n",
    "plt.imshow(image.permute(1,2,0))\n",
    "plt.show()\n",
    "\n",
    "classes = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1012ad75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n",
      "79\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set= random_split(train_data, [40000,10000])\n",
    "\n",
    "train_loader= DataLoader(train_set, batch_size= 128, shuffle=True)\n",
    "val_loader= DataLoader(val_set, batch_size=128, shuffle=False)\n",
    "test_loader= DataLoader(test_data, batch_size=128, shuffle=False)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7306379",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e89bf0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1= nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1= nn.BatchNorm2d(32)\n",
    "        self.relu1= nn.ReLU()\n",
    "\n",
    "        self.conv2= nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2= nn.BatchNorm2d(64)\n",
    "        self.relu2= nn.ReLU()\n",
    "\n",
    "        self.conv3= nn.Conv2d(64,128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3= nn.BatchNorm2d(128)\n",
    "        self.relu3= nn.ReLU()\n",
    "\n",
    "        self.conv4= nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn4= nn.BatchNorm2d(256)\n",
    "        self.relu4= nn.ReLU()\n",
    "\n",
    "        self.conv5= nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5= nn.BatchNorm2d(512)\n",
    "        self.relu5= nn.ReLU()\n",
    "\n",
    "        self.gap= nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten= nn.Flatten()\n",
    "        self.drop= nn.Dropout(0.2)\n",
    "        self.fc1= nn.Linear(512,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))\n",
    "        x = self.relu2(self.bn2(self.conv2(x)))\n",
    "        x = self.relu3(self.bn3(self.conv3(x)))\n",
    "        x = self.relu4(self.bn4(self.conv4(x)))\n",
    "        x = self.relu5(self.bn5(self.conv5(x)))\n",
    "\n",
    "        x = self.gap(x)        \n",
    "        x = self.flatten(x)  \n",
    "        x=  self.drop(x)\n",
    "        x = self.fc1(x)         \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d69ffd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/200] | batch [100/313] | loss : 1.6191601753234863 \n",
      "epoch [1/200] | batch [200/313] | loss : 1.4436675310134888 \n",
      "epoch [1/200] | batch [300/313] | loss : 1.236798882484436 \n",
      "epoch 1/200 avg train loss: 1.5108279713426536 \n",
      "epoch 1/200 avg val loss: 1.363826723792885 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aayam/micromamba/envs/mnist/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [2/200] | batch [100/313] | loss : 1.250151515007019 \n",
      "epoch [2/200] | batch [200/313] | loss : 1.1286802291870117 \n",
      "epoch [2/200] | batch [300/313] | loss : 1.1389622688293457 \n",
      "epoch 2/200 avg train loss: 1.1842747262110724 \n",
      "epoch 2/200 avg val loss: 1.291332483291626 \n",
      "epoch [3/200] | batch [100/313] | loss : 1.0764317512512207 \n",
      "epoch [3/200] | batch [200/313] | loss : 0.9372660517692566 \n",
      "epoch [3/200] | batch [300/313] | loss : 1.1436570882797241 \n",
      "epoch 3/200 avg train loss: 1.0435938469518107 \n",
      "epoch 3/200 avg val loss: 1.067716294451605 \n",
      "epoch [4/200] | batch [100/313] | loss : 0.8879790902137756 \n",
      "epoch [4/200] | batch [200/313] | loss : 1.0151183605194092 \n",
      "epoch [4/200] | batch [300/313] | loss : 0.8484228253364563 \n",
      "epoch 4/200 avg train loss: 0.9506553754258079 \n",
      "epoch 4/200 avg val loss: 1.0942514282238633 \n",
      "epoch [5/200] | batch [100/313] | loss : 0.9687806963920593 \n",
      "epoch [5/200] | batch [200/313] | loss : 1.0601152181625366 \n",
      "epoch [5/200] | batch [300/313] | loss : 0.7657238841056824 \n",
      "epoch 5/200 avg train loss: 0.875755098109809 \n",
      "epoch 5/200 avg val loss: 0.8880727494819255 \n",
      "epoch [6/200] | batch [100/313] | loss : 0.8426281213760376 \n",
      "epoch [6/200] | batch [200/313] | loss : 0.7751389145851135 \n",
      "epoch [6/200] | batch [300/313] | loss : 0.74204021692276 \n",
      "epoch 6/200 avg train loss: 0.8115285745444009 \n",
      "epoch 6/200 avg val loss: 0.8789539276799069 \n",
      "epoch [7/200] | batch [100/313] | loss : 0.7023625373840332 \n",
      "epoch [7/200] | batch [200/313] | loss : 0.7058994174003601 \n",
      "epoch [7/200] | batch [300/313] | loss : 0.7625507712364197 \n",
      "epoch 7/200 avg train loss: 0.7683416909684008 \n",
      "epoch 7/200 avg val loss: 0.7670081295544589 \n",
      "epoch [8/200] | batch [100/313] | loss : 0.5979884266853333 \n",
      "epoch [8/200] | batch [200/313] | loss : 0.6816396117210388 \n",
      "epoch [8/200] | batch [300/313] | loss : 0.5575947761535645 \n",
      "epoch 8/200 avg train loss: 0.7288477174199808 \n",
      "epoch 8/200 avg val loss: 0.7390448005893563 \n",
      "epoch [9/200] | batch [100/313] | loss : 0.6486727595329285 \n",
      "epoch [9/200] | batch [200/313] | loss : 0.731543779373169 \n",
      "epoch [9/200] | batch [300/313] | loss : 0.7353816628456116 \n",
      "epoch 9/200 avg train loss: 0.6939300263461213 \n",
      "epoch 9/200 avg val loss: 0.7299022274681285 \n",
      "epoch [10/200] | batch [100/313] | loss : 0.6307799220085144 \n",
      "epoch [10/200] | batch [200/313] | loss : 0.6360611915588379 \n",
      "epoch [10/200] | batch [300/313] | loss : 0.6384133696556091 \n",
      "epoch 10/200 avg train loss: 0.6604573470525468 \n",
      "epoch 10/200 avg val loss: 0.7476396945458424 \n",
      "epoch [11/200] | batch [100/313] | loss : 0.73708176612854 \n",
      "epoch [11/200] | batch [200/313] | loss : 0.6740460395812988 \n",
      "epoch [11/200] | batch [300/313] | loss : 0.5508397221565247 \n",
      "epoch 11/200 avg train loss: 0.6336390923577756 \n",
      "epoch 11/200 avg val loss: 0.6586492238919947 \n",
      "epoch [12/200] | batch [100/313] | loss : 0.5306001305580139 \n",
      "epoch [12/200] | batch [200/313] | loss : 0.7126779556274414 \n",
      "epoch [12/200] | batch [300/313] | loss : 0.6028874516487122 \n",
      "epoch 12/200 avg train loss: 0.6065827102516406 \n",
      "epoch 12/200 avg val loss: 0.7908892095843448 \n",
      "epoch [13/200] | batch [100/313] | loss : 0.5699769258499146 \n",
      "epoch [13/200] | batch [200/313] | loss : 0.7321534156799316 \n",
      "epoch [13/200] | batch [300/313] | loss : 0.4915287494659424 \n",
      "epoch 13/200 avg train loss: 0.588583281055426 \n",
      "epoch 13/200 avg val loss: 0.640684533722793 \n",
      "epoch [14/200] | batch [100/313] | loss : 0.47588473558425903 \n",
      "epoch [14/200] | batch [200/313] | loss : 0.4742010533809662 \n",
      "epoch [14/200] | batch [300/313] | loss : 0.7262147068977356 \n",
      "epoch 14/200 avg train loss: 0.5693432192642468 \n",
      "epoch 14/200 avg val loss: 0.6966461422322672 \n",
      "epoch [15/200] | batch [100/313] | loss : 0.5657142996788025 \n",
      "epoch [15/200] | batch [200/313] | loss : 0.5174285769462585 \n",
      "epoch [15/200] | batch [300/313] | loss : 0.6297804713249207 \n",
      "epoch 15/200 avg train loss: 0.5507682344783991 \n",
      "epoch 15/200 avg val loss: 0.6097293477269667 \n",
      "epoch [16/200] | batch [100/313] | loss : 0.6360523700714111 \n",
      "epoch [16/200] | batch [200/313] | loss : 0.5307487845420837 \n",
      "epoch [16/200] | batch [300/313] | loss : 0.6359474658966064 \n",
      "epoch 16/200 avg train loss: 0.5338104524361059 \n",
      "epoch 16/200 avg val loss: 0.6542293033267879 \n",
      "epoch [17/200] | batch [100/313] | loss : 0.5016623735427856 \n",
      "epoch [17/200] | batch [200/313] | loss : 0.4148016571998596 \n",
      "epoch [17/200] | batch [300/313] | loss : 0.5680860877037048 \n",
      "epoch 17/200 avg train loss: 0.5159452799410104 \n",
      "epoch 17/200 avg val loss: 0.6098756816568254 \n",
      "epoch [18/200] | batch [100/313] | loss : 0.4787897765636444 \n",
      "epoch [18/200] | batch [200/313] | loss : 0.40659743547439575 \n",
      "epoch [18/200] | batch [300/313] | loss : 0.4710964560508728 \n",
      "epoch 18/200 avg train loss: 0.49931071522517706 \n",
      "epoch 18/200 avg val loss: 0.6850523469568808 \n",
      "epoch [19/200] | batch [100/313] | loss : 0.38638725876808167 \n",
      "epoch [19/200] | batch [200/313] | loss : 0.476147323846817 \n",
      "epoch [19/200] | batch [300/313] | loss : 0.6272675395011902 \n",
      "epoch 19/200 avg train loss: 0.4879197076486703 \n",
      "epoch 19/200 avg val loss: 0.5649846373479578 \n",
      "epoch [20/200] | batch [100/313] | loss : 0.5576897263526917 \n",
      "epoch [20/200] | batch [200/313] | loss : 0.49964722990989685 \n",
      "epoch [20/200] | batch [300/313] | loss : 0.5554065108299255 \n",
      "epoch 20/200 avg train loss: 0.4744398807184384 \n",
      "epoch 20/200 avg val loss: 0.5874520781673963 \n",
      "epoch [21/200] | batch [100/313] | loss : 0.3671085238456726 \n",
      "epoch [21/200] | batch [200/313] | loss : 0.4299655854701996 \n",
      "epoch [21/200] | batch [300/313] | loss : 0.3449707627296448 \n",
      "epoch 21/200 avg train loss: 0.4635169314690672 \n",
      "epoch 21/200 avg val loss: 0.5875614730617668 \n",
      "epoch [22/200] | batch [100/313] | loss : 0.36542922258377075 \n",
      "epoch [22/200] | batch [200/313] | loss : 0.569012463092804 \n",
      "epoch [22/200] | batch [300/313] | loss : 0.5365908741950989 \n",
      "epoch 22/200 avg train loss: 0.4495301261401405 \n",
      "epoch 22/200 avg val loss: 0.5182341993609562 \n",
      "epoch [23/200] | batch [100/313] | loss : 0.3475368618965149 \n",
      "epoch [23/200] | batch [200/313] | loss : 0.4272770285606384 \n",
      "epoch [23/200] | batch [300/313] | loss : 0.6368988752365112 \n",
      "epoch 23/200 avg train loss: 0.4390029247386006 \n",
      "epoch 23/200 avg val loss: 0.5954442906983292 \n",
      "epoch [24/200] | batch [100/313] | loss : 0.3629838228225708 \n",
      "epoch [24/200] | batch [200/313] | loss : 0.4969801604747772 \n",
      "epoch [24/200] | batch [300/313] | loss : 0.3878176808357239 \n",
      "epoch 24/200 avg train loss: 0.43247666317053113 \n",
      "epoch 24/200 avg val loss: 0.5200462341308594 \n",
      "epoch [25/200] | batch [100/313] | loss : 0.4955577552318573 \n",
      "epoch [25/200] | batch [200/313] | loss : 0.333493709564209 \n",
      "epoch [25/200] | batch [300/313] | loss : 0.4292669892311096 \n",
      "epoch 25/200 avg train loss: 0.42273638176080136 \n",
      "epoch 25/200 avg val loss: 0.5255908471874043 \n",
      "epoch [26/200] | batch [100/313] | loss : 0.5048311948776245 \n",
      "epoch [26/200] | batch [200/313] | loss : 0.42476075887680054 \n",
      "epoch [26/200] | batch [300/313] | loss : 0.35117852687835693 \n",
      "epoch 26/200 avg train loss: 0.40860082323368363 \n",
      "epoch 26/200 avg val loss: 0.529983377532114 \n",
      "epoch [27/200] | batch [100/313] | loss : 0.4609610438346863 \n",
      "epoch [27/200] | batch [200/313] | loss : 0.4713955223560333 \n",
      "epoch [27/200] | batch [300/313] | loss : 0.35078778862953186 \n",
      "epoch 27/200 avg train loss: 0.3962844361702855 \n",
      "epoch 27/200 avg val loss: 0.5684423095817808 \n",
      "epoch [28/200] | batch [100/313] | loss : 0.39588335156440735 \n",
      "epoch [28/200] | batch [200/313] | loss : 0.34033405780792236 \n",
      "epoch [28/200] | batch [300/313] | loss : 0.47234684228897095 \n",
      "epoch 28/200 avg train loss: 0.3886233541531304 \n",
      "epoch 28/200 avg val loss: 0.5597355082819734 \n",
      "epoch [29/200] | batch [100/313] | loss : 0.3355022668838501 \n",
      "epoch [29/200] | batch [200/313] | loss : 0.3611641228199005 \n",
      "epoch [29/200] | batch [300/313] | loss : 0.39271095395088196 \n",
      "epoch 29/200 avg train loss: 0.3905950879898315 \n",
      "epoch 29/200 avg val loss: 0.6160355346112312 \n",
      "epoch [30/200] | batch [100/313] | loss : 0.3854302763938904 \n",
      "epoch [30/200] | batch [200/313] | loss : 0.40161117911338806 \n",
      "epoch [30/200] | batch [300/313] | loss : 0.3447619080543518 \n",
      "epoch 30/200 avg train loss: 0.3777968249857997 \n",
      "epoch 30/200 avg val loss: 0.525635832095448 \n",
      "epoch [31/200] | batch [100/313] | loss : 0.24282141029834747 \n",
      "epoch [31/200] | batch [200/313] | loss : 0.4237700402736664 \n",
      "epoch [31/200] | batch [300/313] | loss : 0.34071868658065796 \n",
      "epoch 31/200 avg train loss: 0.36897871997981024 \n",
      "epoch 31/200 avg val loss: 0.5203294120257413 \n",
      "epoch [32/200] | batch [100/313] | loss : 0.45743221044540405 \n",
      "epoch [32/200] | batch [200/313] | loss : 0.3394489586353302 \n",
      "epoch [32/200] | batch [300/313] | loss : 0.3479122519493103 \n",
      "epoch 32/200 avg train loss: 0.35741005747462995 \n",
      "epoch 32/200 avg val loss: 0.5335184203673012 \n",
      "epoch [33/200] | batch [100/313] | loss : 0.4061605930328369 \n",
      "epoch [33/200] | batch [200/313] | loss : 0.4198923110961914 \n",
      "epoch [33/200] | batch [300/313] | loss : 0.37742045521736145 \n",
      "epoch 33/200 avg train loss: 0.351138583673075 \n",
      "epoch 33/200 avg val loss: 0.4951231634315056 \n",
      "epoch [34/200] | batch [100/313] | loss : 0.34693315625190735 \n",
      "epoch [34/200] | batch [200/313] | loss : 0.31303533911705017 \n",
      "epoch [34/200] | batch [300/313] | loss : 0.5470724701881409 \n",
      "epoch 34/200 avg train loss: 0.3475809989455409 \n",
      "epoch 34/200 avg val loss: 0.49381094642832307 \n",
      "epoch [35/200] | batch [100/313] | loss : 0.21814271807670593 \n",
      "epoch [35/200] | batch [200/313] | loss : 0.20502106845378876 \n",
      "epoch [35/200] | batch [300/313] | loss : 0.32597941160202026 \n",
      "epoch 35/200 avg train loss: 0.3392893048330618 \n",
      "epoch 35/200 avg val loss: 0.5127746983419491 \n",
      "epoch [36/200] | batch [100/313] | loss : 0.3716917037963867 \n",
      "epoch [36/200] | batch [200/313] | loss : 0.34824174642562866 \n",
      "epoch [36/200] | batch [300/313] | loss : 0.35910969972610474 \n",
      "epoch 36/200 avg train loss: 0.3338719277907484 \n",
      "epoch 36/200 avg val loss: 0.49644888390468644 \n",
      "epoch [37/200] | batch [100/313] | loss : 0.3062765300273895 \n",
      "epoch [37/200] | batch [200/313] | loss : 0.3997746407985687 \n",
      "epoch [37/200] | batch [300/313] | loss : 0.3611385226249695 \n",
      "epoch 37/200 avg train loss: 0.3252778452710983 \n",
      "epoch 37/200 avg val loss: 0.5229056545450718 \n",
      "epoch [38/200] | batch [100/313] | loss : 0.4151526093482971 \n",
      "epoch [38/200] | batch [200/313] | loss : 0.31555116176605225 \n",
      "epoch [38/200] | batch [300/313] | loss : 0.32635918259620667 \n",
      "epoch 38/200 avg train loss: 0.32846934150773494 \n",
      "epoch 38/200 avg val loss: 0.5192752597452719 \n",
      "epoch [39/200] | batch [100/313] | loss : 0.32312169671058655 \n",
      "epoch [39/200] | batch [200/313] | loss : 0.4393770396709442 \n",
      "epoch [39/200] | batch [300/313] | loss : 0.5626562833786011 \n",
      "epoch 39/200 avg train loss: 0.31339625066842514 \n",
      "epoch 39/200 avg val loss: 0.518428824747665 \n",
      "epoch [40/200] | batch [100/313] | loss : 0.23124158382415771 \n",
      "epoch [40/200] | batch [200/313] | loss : 0.3100724518299103 \n",
      "epoch [40/200] | batch [300/313] | loss : 0.26976415514945984 \n",
      "epoch 40/200 avg train loss: 0.30939067866855535 \n",
      "epoch 40/200 avg val loss: 0.4925772347782232 \n",
      "epoch [41/200] | batch [100/313] | loss : 0.3210391104221344 \n",
      "epoch [41/200] | batch [200/313] | loss : 0.351169228553772 \n",
      "epoch [41/200] | batch [300/313] | loss : 0.21290329098701477 \n",
      "epoch 41/200 avg train loss: 0.30768118893947843 \n",
      "epoch 41/200 avg val loss: 0.49908914086939415 \n",
      "epoch [42/200] | batch [100/313] | loss : 0.2951606810092926 \n",
      "epoch [42/200] | batch [200/313] | loss : 0.2633889615535736 \n",
      "epoch [42/200] | batch [300/313] | loss : 0.1821656972169876 \n",
      "epoch 42/200 avg train loss: 0.29642250786383695 \n",
      "epoch 42/200 avg val loss: 0.5004438516459887 \n",
      "epoch [43/200] | batch [100/313] | loss : 0.255176842212677 \n",
      "epoch [43/200] | batch [200/313] | loss : 0.3088604211807251 \n",
      "epoch [43/200] | batch [300/313] | loss : 0.29241278767585754 \n",
      "epoch 43/200 avg train loss: 0.29067793440894957 \n",
      "epoch 43/200 avg val loss: 0.5028975679527355 \n",
      "epoch [44/200] | batch [100/313] | loss : 0.2567112445831299 \n",
      "epoch [44/200] | batch [200/313] | loss : 0.3024248480796814 \n",
      "epoch [44/200] | batch [300/313] | loss : 0.26945921778678894 \n",
      "epoch 44/200 avg train loss: 0.2900461181046102 \n",
      "epoch 44/200 avg val loss: 0.4395992156070999 \n",
      "epoch [45/200] | batch [100/313] | loss : 0.18928676843643188 \n",
      "epoch [45/200] | batch [200/313] | loss : 0.2727733552455902 \n",
      "epoch [45/200] | batch [300/313] | loss : 0.3208201229572296 \n",
      "epoch 45/200 avg train loss: 0.28268804260717034 \n",
      "epoch 45/200 avg val loss: 0.49390388025513177 \n",
      "epoch [46/200] | batch [100/313] | loss : 0.2435084879398346 \n",
      "epoch [46/200] | batch [200/313] | loss : 0.22977781295776367 \n",
      "epoch [46/200] | batch [300/313] | loss : 0.2655179500579834 \n",
      "epoch 46/200 avg train loss: 0.27992297918461384 \n",
      "epoch 46/200 avg val loss: 0.4707802828354172 \n",
      "epoch [47/200] | batch [100/313] | loss : 0.368781715631485 \n",
      "epoch [47/200] | batch [200/313] | loss : 0.18586941063404083 \n",
      "epoch [47/200] | batch [300/313] | loss : 0.2921309173107147 \n",
      "epoch 47/200 avg train loss: 0.2755643317398553 \n",
      "epoch 47/200 avg val loss: 0.45060439351238785 \n",
      "epoch [48/200] | batch [100/313] | loss : 0.3538181185722351 \n",
      "epoch [48/200] | batch [200/313] | loss : 0.3488960266113281 \n",
      "epoch [48/200] | batch [300/313] | loss : 0.24623118340969086 \n",
      "epoch 48/200 avg train loss: 0.26785842879130817 \n",
      "epoch 48/200 avg val loss: 0.49614515259296077 \n",
      "epoch [49/200] | batch [100/313] | loss : 0.24912628531455994 \n",
      "epoch [49/200] | batch [200/313] | loss : 0.2380799502134323 \n",
      "epoch [49/200] | batch [300/313] | loss : 0.2961690127849579 \n",
      "epoch 49/200 avg train loss: 0.264711394382361 \n",
      "epoch 49/200 avg val loss: 0.4866305216203762 \n",
      "epoch [50/200] | batch [100/313] | loss : 0.3597647547721863 \n",
      "epoch [50/200] | batch [200/313] | loss : 0.3585444390773773 \n",
      "epoch [50/200] | batch [300/313] | loss : 0.3202197551727295 \n",
      "epoch 50/200 avg train loss: 0.26174934979635306 \n",
      "epoch 50/200 avg val loss: 0.4947424066972129 \n",
      "epoch [51/200] | batch [100/313] | loss : 0.24058349430561066 \n",
      "epoch [51/200] | batch [200/313] | loss : 0.3494167625904083 \n",
      "epoch [51/200] | batch [300/313] | loss : 0.3408072292804718 \n",
      "epoch 51/200 avg train loss: 0.26062164205712635 \n",
      "epoch 51/200 avg val loss: 0.5388511166542391 \n",
      "epoch [52/200] | batch [100/313] | loss : 0.21745765209197998 \n",
      "epoch [52/200] | batch [200/313] | loss : 0.25046613812446594 \n",
      "epoch [52/200] | batch [300/313] | loss : 0.3028844892978668 \n",
      "epoch 52/200 avg train loss: 0.2526411625523918 \n",
      "epoch 52/200 avg val loss: 0.5504331811319424 \n",
      "epoch [53/200] | batch [100/313] | loss : 0.2582111060619354 \n",
      "epoch [53/200] | batch [200/313] | loss : 0.24318234622478485 \n",
      "epoch [53/200] | batch [300/313] | loss : 0.17993532121181488 \n",
      "epoch 53/200 avg train loss: 0.2494470564225992 \n",
      "epoch 53/200 avg val loss: 0.48293675463410873 \n",
      "epoch [54/200] | batch [100/313] | loss : 0.18880695104599 \n",
      "epoch [54/200] | batch [200/313] | loss : 0.25602641701698303 \n",
      "epoch [54/200] | batch [300/313] | loss : 0.3320167064666748 \n",
      "epoch 54/200 avg train loss: 0.23850469748242595 \n",
      "epoch 54/200 avg val loss: 0.4619474150711977 \n",
      "epoch [55/200] | batch [100/313] | loss : 0.27660295367240906 \n",
      "epoch [55/200] | batch [200/313] | loss : 0.23218531906604767 \n",
      "epoch [55/200] | batch [300/313] | loss : 0.2683270573616028 \n",
      "epoch 55/200 avg train loss: 0.2398250499329628 \n",
      "epoch 55/200 avg val loss: 0.45267962956730323 \n",
      "epoch [56/200] | batch [100/313] | loss : 0.2155040204524994 \n",
      "epoch [56/200] | batch [200/313] | loss : 0.3402421772480011 \n",
      "epoch [56/200] | batch [300/313] | loss : 0.15668708086013794 \n",
      "epoch 56/200 avg train loss: 0.23360447856945732 \n",
      "epoch 56/200 avg val loss: 0.5163610664349568 \n",
      "epoch [57/200] | batch [100/313] | loss : 0.1770370453596115 \n",
      "epoch [57/200] | batch [200/313] | loss : 0.31453993916511536 \n",
      "epoch [57/200] | batch [300/313] | loss : 0.1259736865758896 \n",
      "epoch 57/200 avg train loss: 0.2304943087525642 \n",
      "epoch 57/200 avg val loss: 0.5001622969402543 \n",
      "epoch [58/200] | batch [100/313] | loss : 0.20900081098079681 \n",
      "epoch [58/200] | batch [200/313] | loss : 0.27888593077659607 \n",
      "epoch [58/200] | batch [300/313] | loss : 0.25627538561820984 \n",
      "epoch 58/200 avg train loss: 0.23140492295019163 \n",
      "epoch 58/200 avg val loss: 0.4622982597049278 \n",
      "epoch [59/200] | batch [100/313] | loss : 0.2510278522968292 \n",
      "epoch [59/200] | batch [200/313] | loss : 0.1916389763355255 \n",
      "epoch [59/200] | batch [300/313] | loss : 0.2632017135620117 \n",
      "epoch 59/200 avg train loss: 0.23154328658748358 \n",
      "epoch 59/200 avg val loss: 0.47226615004901645 \n",
      "epoch [60/200] | batch [100/313] | loss : 0.26386258006095886 \n",
      "epoch [60/200] | batch [200/313] | loss : 0.22900964319705963 \n",
      "epoch [60/200] | batch [300/313] | loss : 0.27038806676864624 \n",
      "epoch 60/200 avg train loss: 0.2219245917976093 \n",
      "epoch 60/200 avg val loss: 0.46099228345895116 \n",
      "epoch [61/200] | batch [100/313] | loss : 0.1786004900932312 \n",
      "epoch [61/200] | batch [200/313] | loss : 0.1694536805152893 \n",
      "epoch [61/200] | batch [300/313] | loss : 0.20682820677757263 \n",
      "epoch 61/200 avg train loss: 0.2212702697410751 \n",
      "epoch 61/200 avg val loss: 0.4958611939904056 \n",
      "epoch [62/200] | batch [100/313] | loss : 0.198868989944458 \n",
      "epoch [62/200] | batch [200/313] | loss : 0.2708061933517456 \n",
      "epoch [62/200] | batch [300/313] | loss : 0.2579675316810608 \n",
      "epoch 62/200 avg train loss: 0.21637864865529272 \n",
      "epoch 62/200 avg val loss: 0.44731427012365077 \n",
      "epoch [63/200] | batch [100/313] | loss : 0.22214603424072266 \n",
      "epoch [63/200] | batch [200/313] | loss : 0.20544536411762238 \n",
      "epoch [63/200] | batch [300/313] | loss : 0.20802855491638184 \n",
      "epoch 63/200 avg train loss: 0.2149074800526753 \n",
      "epoch 63/200 avg val loss: 0.4910907834013806 \n",
      "epoch [64/200] | batch [100/313] | loss : 0.22086970508098602 \n",
      "epoch [64/200] | batch [200/313] | loss : 0.273235946893692 \n",
      "epoch [64/200] | batch [300/313] | loss : 0.3124886453151703 \n",
      "epoch 64/200 avg train loss: 0.21348066275683455 \n",
      "epoch 64/200 avg val loss: 0.46280682143531265 \n",
      "epoch [65/200] | batch [100/313] | loss : 0.1499151587486267 \n",
      "epoch [65/200] | batch [200/313] | loss : 0.2717756927013397 \n",
      "epoch [65/200] | batch [300/313] | loss : 0.1274910718202591 \n",
      "epoch 65/200 avg train loss: 0.20382318864710414 \n",
      "epoch 65/200 avg val loss: 0.48105731172652183 \n",
      "epoch [66/200] | batch [100/313] | loss : 0.2282850295305252 \n",
      "epoch [66/200] | batch [200/313] | loss : 0.2608207166194916 \n",
      "epoch [66/200] | batch [300/313] | loss : 0.2722170352935791 \n",
      "epoch 66/200 avg train loss: 0.20430863205902874 \n",
      "epoch 66/200 avg val loss: 0.506719284419772 \n",
      "epoch [67/200] | batch [100/313] | loss : 0.24552804231643677 \n",
      "epoch [67/200] | batch [200/313] | loss : 0.23349329829216003 \n",
      "epoch [67/200] | batch [300/313] | loss : 0.2831866443157196 \n",
      "epoch 67/200 avg train loss: 0.20400578797625277 \n",
      "epoch 67/200 avg val loss: 0.44539734079867976 \n",
      "epoch [68/200] | batch [100/313] | loss : 0.1445721983909607 \n",
      "epoch [68/200] | batch [200/313] | loss : 0.3047792613506317 \n",
      "epoch [68/200] | batch [300/313] | loss : 0.15896426141262054 \n",
      "epoch 68/200 avg train loss: 0.1989585593247566 \n",
      "epoch 68/200 avg val loss: 0.4612959557318989 \n",
      "epoch [69/200] | batch [100/313] | loss : 0.2545051574707031 \n",
      "epoch [69/200] | batch [200/313] | loss : 0.13550715148448944 \n",
      "epoch [69/200] | batch [300/313] | loss : 0.18351705372333527 \n",
      "epoch 69/200 avg train loss: 0.19854661596183198 \n",
      "epoch 69/200 avg val loss: 0.5270854504425314 \n",
      "epoch [70/200] | batch [100/313] | loss : 0.22141507267951965 \n",
      "epoch [70/200] | batch [200/313] | loss : 0.07269027829170227 \n",
      "epoch [70/200] | batch [300/313] | loss : 0.3968774676322937 \n",
      "epoch 70/200 avg train loss: 0.1902165140349644 \n",
      "epoch 70/200 avg val loss: 0.4676753331588793 \n",
      "epoch [71/200] | batch [100/313] | loss : 0.23766879737377167 \n",
      "epoch [71/200] | batch [200/313] | loss : 0.16849175095558167 \n",
      "epoch [71/200] | batch [300/313] | loss : 0.26454341411590576 \n",
      "epoch 71/200 avg train loss: 0.19123261621870552 \n",
      "epoch 71/200 avg val loss: 0.47028461475915545 \n",
      "epoch [72/200] | batch [100/313] | loss : 0.1395666003227234 \n",
      "epoch [72/200] | batch [200/313] | loss : 0.14697609841823578 \n",
      "epoch [72/200] | batch [300/313] | loss : 0.15931735932826996 \n",
      "epoch 72/200 avg train loss: 0.1849706148663268 \n",
      "epoch 72/200 avg val loss: 0.48008330090890955 \n",
      "epoch [73/200] | batch [100/313] | loss : 0.18978552520275116 \n",
      "epoch [73/200] | batch [200/313] | loss : 0.17086845636367798 \n",
      "epoch [73/200] | batch [300/313] | loss : 0.1977471113204956 \n",
      "epoch 73/200 avg train loss: 0.18722297789189762 \n",
      "epoch 73/200 avg val loss: 0.4361992118102086 \n",
      "epoch [74/200] | batch [100/313] | loss : 0.1352618783712387 \n",
      "epoch [74/200] | batch [200/313] | loss : 0.13483065366744995 \n",
      "epoch [74/200] | batch [300/313] | loss : 0.13638746738433838 \n",
      "epoch 74/200 avg train loss: 0.18020518752523124 \n",
      "epoch 74/200 avg val loss: 0.4446614333723165 \n",
      "epoch [75/200] | batch [100/313] | loss : 0.13071730732917786 \n",
      "epoch [75/200] | batch [200/313] | loss : 0.23795519769191742 \n",
      "epoch [75/200] | batch [300/313] | loss : 0.21306747198104858 \n",
      "epoch 75/200 avg train loss: 0.17676495936827158 \n",
      "epoch 75/200 avg val loss: 0.46606844682482224 \n",
      "epoch [76/200] | batch [100/313] | loss : 0.19308063387870789 \n",
      "epoch [76/200] | batch [200/313] | loss : 0.13438007235527039 \n",
      "epoch [76/200] | batch [300/313] | loss : 0.2117481678724289 \n",
      "epoch 76/200 avg train loss: 0.17603491677548558 \n",
      "epoch 76/200 avg val loss: 0.4853150016522106 \n",
      "epoch [77/200] | batch [100/313] | loss : 0.2290528416633606 \n",
      "epoch [77/200] | batch [200/313] | loss : 0.2633455693721771 \n",
      "epoch [77/200] | batch [300/313] | loss : 0.08956681936979294 \n",
      "epoch 77/200 avg train loss: 0.17305314212370984 \n",
      "epoch 77/200 avg val loss: 0.46883013995387884 \n",
      "epoch [78/200] | batch [100/313] | loss : 0.1556631326675415 \n",
      "epoch [78/200] | batch [200/313] | loss : 0.16789324581623077 \n",
      "epoch [78/200] | batch [300/313] | loss : 0.16550993919372559 \n",
      "epoch 78/200 avg train loss: 0.17473468848596366 \n",
      "epoch 78/200 avg val loss: 0.5115409810331804 \n",
      "epoch [79/200] | batch [100/313] | loss : 0.12787112593650818 \n",
      "epoch [79/200] | batch [200/313] | loss : 0.18414582312107086 \n",
      "epoch [79/200] | batch [300/313] | loss : 0.14860880374908447 \n",
      "epoch 79/200 avg train loss: 0.16943075360295873 \n",
      "epoch 79/200 avg val loss: 0.4798019843388207 \n",
      "epoch [80/200] | batch [100/313] | loss : 0.14496523141860962 \n",
      "epoch [80/200] | batch [200/313] | loss : 0.17557448148727417 \n",
      "epoch [80/200] | batch [300/313] | loss : 0.14994965493679047 \n",
      "epoch 80/200 avg train loss: 0.17087269726747903 \n",
      "epoch 80/200 avg val loss: 0.4718550492691088 \n",
      "epoch [81/200] | batch [100/313] | loss : 0.0965050607919693 \n",
      "epoch [81/200] | batch [200/313] | loss : 0.08801721036434174 \n",
      "epoch [81/200] | batch [300/313] | loss : 0.2150072306394577 \n",
      "epoch 81/200 avg train loss: 0.16616755812027204 \n",
      "epoch 81/200 avg val loss: 0.4651738676843764 \n",
      "epoch [82/200] | batch [100/313] | loss : 0.16775016486644745 \n",
      "epoch [82/200] | batch [200/313] | loss : 0.19754472374916077 \n",
      "epoch [82/200] | batch [300/313] | loss : 0.16054247319698334 \n",
      "epoch 82/200 avg train loss: 0.16426264501798649 \n",
      "epoch 82/200 avg val loss: 0.5180700251573249 \n",
      "epoch [83/200] | batch [100/313] | loss : 0.21133658289909363 \n",
      "epoch [83/200] | batch [200/313] | loss : 0.16666778922080994 \n",
      "epoch [83/200] | batch [300/313] | loss : 0.10369992256164551 \n",
      "epoch 83/200 avg train loss: 0.1592977826302044 \n",
      "epoch 83/200 avg val loss: 0.48834395710426043 \n",
      "epoch [84/200] | batch [100/313] | loss : 0.131600484251976 \n",
      "epoch [84/200] | batch [200/313] | loss : 0.1774834841489792 \n",
      "epoch [84/200] | batch [300/313] | loss : 0.22413921356201172 \n",
      "epoch 84/200 avg train loss: 0.15762400700927923 \n",
      "epoch 84/200 avg val loss: 0.5248233634459821 \n",
      "epoch [85/200] | batch [100/313] | loss : 0.11683425307273865 \n",
      "epoch [85/200] | batch [200/313] | loss : 0.2234552800655365 \n",
      "epoch [85/200] | batch [300/313] | loss : 0.20309500396251678 \n",
      "epoch 85/200 avg train loss: 0.15667267484120287 \n",
      "epoch 85/200 avg val loss: 0.47816486543492426 \n",
      "epoch [86/200] | batch [100/313] | loss : 0.27461546659469604 \n",
      "epoch [86/200] | batch [200/313] | loss : 0.11748060584068298 \n",
      "epoch [86/200] | batch [300/313] | loss : 0.12542912364006042 \n",
      "epoch 86/200 avg train loss: 0.15875366124244164 \n",
      "epoch 86/200 avg val loss: 0.4644423219599301 \n",
      "epoch [87/200] | batch [100/313] | loss : 0.13656215369701385 \n",
      "epoch [87/200] | batch [200/313] | loss : 0.3443473279476166 \n",
      "epoch [87/200] | batch [300/313] | loss : 0.17196111381053925 \n",
      "epoch 87/200 avg train loss: 0.14925223553237824 \n",
      "epoch 87/200 avg val loss: 0.5022512455529804 \n",
      "epoch [88/200] | batch [100/313] | loss : 0.12963323295116425 \n",
      "epoch [88/200] | batch [200/313] | loss : 0.12226957827806473 \n",
      "epoch [88/200] | batch [300/313] | loss : 0.21915532648563385 \n",
      "epoch 88/200 avg train loss: 0.15491222540220131 \n",
      "epoch 88/200 avg val loss: 0.480049192905426 \n",
      "epoch [89/200] | batch [100/313] | loss : 0.16346228122711182 \n",
      "epoch [89/200] | batch [200/313] | loss : 0.17700408399105072 \n",
      "epoch [89/200] | batch [300/313] | loss : 0.15455321967601776 \n",
      "epoch 89/200 avg train loss: 0.14939806452050758 \n",
      "epoch 89/200 avg val loss: 0.5212309390683717 \n",
      "epoch [90/200] | batch [100/313] | loss : 0.11664288491010666 \n",
      "epoch [90/200] | batch [200/313] | loss : 0.18248628079891205 \n",
      "epoch [90/200] | batch [300/313] | loss : 0.15187756717205048 \n",
      "epoch 90/200 avg train loss: 0.1444647339776682 \n",
      "epoch 90/200 avg val loss: 0.4933575338955167 \n",
      "epoch [91/200] | batch [100/313] | loss : 0.18913696706295013 \n",
      "epoch [91/200] | batch [200/313] | loss : 0.10687649995088577 \n",
      "epoch [91/200] | batch [300/313] | loss : 0.17205536365509033 \n",
      "epoch 91/200 avg train loss: 0.1456922993135338 \n",
      "epoch 91/200 avg val loss: 0.528534096818936 \n",
      "epoch [92/200] | batch [100/313] | loss : 0.11602885276079178 \n",
      "epoch [92/200] | batch [200/313] | loss : 0.1212754026055336 \n",
      "epoch [92/200] | batch [300/313] | loss : 0.16606181859970093 \n",
      "epoch 92/200 avg train loss: 0.14156028993737202 \n",
      "epoch 92/200 avg val loss: 0.4908465377137631 \n",
      "epoch [93/200] | batch [100/313] | loss : 0.20131047070026398 \n",
      "epoch [93/200] | batch [200/313] | loss : 0.14221082627773285 \n",
      "epoch [93/200] | batch [300/313] | loss : 0.15107037127017975 \n",
      "epoch 93/200 avg train loss: 0.1447451127151521 \n",
      "epoch 93/200 avg val loss: 0.5718038508409187 \n",
      "epoch [94/200] | batch [100/313] | loss : 0.09240589290857315 \n",
      "epoch [94/200] | batch [200/313] | loss : 0.14240291714668274 \n",
      "epoch [94/200] | batch [300/313] | loss : 0.13128694891929626 \n",
      "epoch 94/200 avg train loss: 0.14476036011410978 \n",
      "epoch 94/200 avg val loss: 0.5110736896720114 \n",
      "epoch [95/200] | batch [100/313] | loss : 0.13090640306472778 \n",
      "epoch [95/200] | batch [200/313] | loss : 0.14194251596927643 \n",
      "epoch [95/200] | batch [300/313] | loss : 0.14902937412261963 \n",
      "epoch 95/200 avg train loss: 0.13981242876607008 \n",
      "epoch 95/200 avg val loss: 0.48902784042720554 \n",
      "epoch [96/200] | batch [100/313] | loss : 0.0923832431435585 \n",
      "epoch [96/200] | batch [200/313] | loss : 0.17777487635612488 \n",
      "epoch [96/200] | batch [300/313] | loss : 0.17501530051231384 \n",
      "epoch 96/200 avg train loss: 0.1369824088014924 \n",
      "epoch 96/200 avg val loss: 0.48271672861485543 \n",
      "epoch [97/200] | batch [100/313] | loss : 0.09522959589958191 \n",
      "epoch [97/200] | batch [200/313] | loss : 0.20730583369731903 \n",
      "epoch [97/200] | batch [300/313] | loss : 0.11153783649206161 \n",
      "epoch 97/200 avg train loss: 0.13660054626508641 \n",
      "epoch 97/200 avg val loss: 0.4895333137504662 \n",
      "epoch [98/200] | batch [100/313] | loss : 0.08913517743349075 \n",
      "epoch [98/200] | batch [200/313] | loss : 0.11897577345371246 \n",
      "epoch [98/200] | batch [300/313] | loss : 0.19147711992263794 \n",
      "epoch 98/200 avg train loss: 0.13188918568074895 \n",
      "epoch 98/200 avg val loss: 0.4842999196505245 \n",
      "epoch [99/200] | batch [100/313] | loss : 0.16868194937705994 \n",
      "epoch [99/200] | batch [200/313] | loss : 0.08599915355443954 \n",
      "epoch [99/200] | batch [300/313] | loss : 0.17983104288578033 \n",
      "epoch 99/200 avg train loss: 0.1310193174349043 \n",
      "epoch 99/200 avg val loss: 0.5126200207049334 \n",
      "epoch [100/200] | batch [100/313] | loss : 0.20866647362709045 \n",
      "epoch [100/200] | batch [200/313] | loss : 0.12546628713607788 \n",
      "epoch [100/200] | batch [300/313] | loss : 0.1410825103521347 \n",
      "epoch 100/200 avg train loss: 0.13148347494272758 \n",
      "epoch 100/200 avg val loss: 0.516962083834636 \n",
      "epoch [101/200] | batch [100/313] | loss : 0.10004490613937378 \n",
      "epoch [101/200] | batch [200/313] | loss : 0.11447704583406448 \n",
      "epoch [101/200] | batch [300/313] | loss : 0.1584288626909256 \n",
      "epoch 101/200 avg train loss: 0.1304731460901114 \n",
      "epoch 101/200 avg val loss: 0.526817954793761 \n",
      "epoch [102/200] | batch [100/313] | loss : 0.13961708545684814 \n",
      "epoch [102/200] | batch [200/313] | loss : 0.2538849413394928 \n",
      "epoch [102/200] | batch [300/313] | loss : 0.21739788353443146 \n",
      "epoch 102/200 avg train loss: 0.1263313399383816 \n",
      "epoch 102/200 avg val loss: 0.49085506363005577 \n",
      "epoch [103/200] | batch [100/313] | loss : 0.09529926627874374 \n",
      "epoch [103/200] | batch [200/313] | loss : 0.11037857085466385 \n",
      "epoch [103/200] | batch [300/313] | loss : 0.11441227793693542 \n",
      "epoch 103/200 avg train loss: 0.12476391864184755 \n",
      "epoch 103/200 avg val loss: 0.451343755367436 \n",
      "epoch [104/200] | batch [100/313] | loss : 0.11936246603727341 \n",
      "epoch [104/200] | batch [200/313] | loss : 0.1725904643535614 \n",
      "epoch [104/200] | batch [300/313] | loss : 0.1005350723862648 \n",
      "epoch 104/200 avg train loss: 0.12352147854317111 \n",
      "epoch 104/200 avg val loss: 0.48650755478611474 \n",
      "epoch [105/200] | batch [100/313] | loss : 0.1929616928100586 \n",
      "epoch [105/200] | batch [200/313] | loss : 0.14582069218158722 \n",
      "epoch [105/200] | batch [300/313] | loss : 0.14518561959266663 \n",
      "epoch 105/200 avg train loss: 0.12566039441064142 \n",
      "epoch 105/200 avg val loss: 0.47262419175498094 \n",
      "epoch [106/200] | batch [100/313] | loss : 0.09621431678533554 \n",
      "epoch [106/200] | batch [200/313] | loss : 0.0757947638630867 \n",
      "epoch [106/200] | batch [300/313] | loss : 0.12135138362646103 \n",
      "epoch 106/200 avg train loss: 0.12103368593052553 \n",
      "epoch 106/200 avg val loss: 0.45145364973364 \n",
      "epoch [107/200] | batch [100/313] | loss : 0.13127127289772034 \n",
      "epoch [107/200] | batch [200/313] | loss : 0.22401900589466095 \n",
      "epoch [107/200] | batch [300/313] | loss : 0.1046033650636673 \n",
      "epoch 107/200 avg train loss: 0.12216804287256525 \n",
      "epoch 107/200 avg val loss: 0.4925035985210274 \n",
      "epoch [108/200] | batch [100/313] | loss : 0.0764315277338028 \n",
      "epoch [108/200] | batch [200/313] | loss : 0.2169169932603836 \n",
      "epoch [108/200] | batch [300/313] | loss : 0.12447664886713028 \n",
      "epoch 108/200 avg train loss: 0.12188824884498271 \n",
      "epoch 108/200 avg val loss: 0.49946751926518695 \n",
      "epoch [109/200] | batch [100/313] | loss : 0.1412171572446823 \n",
      "epoch [109/200] | batch [200/313] | loss : 0.10009966045618057 \n",
      "epoch [109/200] | batch [300/313] | loss : 0.08097506314516068 \n",
      "epoch 109/200 avg train loss: 0.11805104409543851 \n",
      "epoch 109/200 avg val loss: 0.4737767482860179 \n",
      "epoch [110/200] | batch [100/313] | loss : 0.1749332994222641 \n",
      "epoch [110/200] | batch [200/313] | loss : 0.059189196676015854 \n",
      "epoch [110/200] | batch [300/313] | loss : 0.10802426189184189 \n",
      "epoch 110/200 avg train loss: 0.11552508250354959 \n",
      "epoch 110/200 avg val loss: 0.5141139762311042 \n",
      "epoch [111/200] | batch [100/313] | loss : 0.16722506284713745 \n",
      "epoch [111/200] | batch [200/313] | loss : 0.17990612983703613 \n",
      "epoch [111/200] | batch [300/313] | loss : 0.14517296850681305 \n",
      "epoch 111/200 avg train loss: 0.11736773983977092 \n",
      "epoch 111/200 avg val loss: 0.49199630565281155 \n",
      "epoch [112/200] | batch [100/313] | loss : 0.08451584726572037 \n",
      "epoch [112/200] | batch [200/313] | loss : 0.14366628229618073 \n",
      "epoch [112/200] | batch [300/313] | loss : 0.1641600877046585 \n",
      "epoch 112/200 avg train loss: 0.11565071813309916 \n",
      "epoch 112/200 avg val loss: 0.5367701230924341 \n",
      "epoch [113/200] | batch [100/313] | loss : 0.08706573396921158 \n",
      "epoch [113/200] | batch [200/313] | loss : 0.1204184964299202 \n",
      "epoch [113/200] | batch [300/313] | loss : 0.08338067680597305 \n",
      "epoch 113/200 avg train loss: 0.11664955125163538 \n",
      "epoch 113/200 avg val loss: 0.4891039391484442 \n",
      "epoch [114/200] | batch [100/313] | loss : 0.0657036229968071 \n",
      "epoch [114/200] | batch [200/313] | loss : 0.09112050384283066 \n",
      "epoch [114/200] | batch [300/313] | loss : 0.15520647168159485 \n",
      "epoch 114/200 avg train loss: 0.10823842421316872 \n",
      "epoch 114/200 avg val loss: 0.49489757248872446 \n",
      "epoch [115/200] | batch [100/313] | loss : 0.08014437556266785 \n",
      "epoch [115/200] | batch [200/313] | loss : 0.10891634225845337 \n",
      "epoch [115/200] | batch [300/313] | loss : 0.09156816452741623 \n",
      "epoch 115/200 avg train loss: 0.11019345541922049 \n",
      "epoch 115/200 avg val loss: 0.49059465602983404 \n",
      "epoch [116/200] | batch [100/313] | loss : 0.1133536621928215 \n",
      "epoch [116/200] | batch [200/313] | loss : 0.09689946472644806 \n",
      "epoch [116/200] | batch [300/313] | loss : 0.056912921369075775 \n",
      "epoch 116/200 avg train loss: 0.10711780159522931 \n",
      "epoch 116/200 avg val loss: 0.48226230223722094 \n",
      "epoch [117/200] | batch [100/313] | loss : 0.05397770553827286 \n",
      "epoch [117/200] | batch [200/313] | loss : 0.11504464596509933 \n",
      "epoch [117/200] | batch [300/313] | loss : 0.10524401068687439 \n",
      "epoch 117/200 avg train loss: 0.10836073211706675 \n",
      "epoch 117/200 avg val loss: 0.4924098379627059 \n",
      "epoch [118/200] | batch [100/313] | loss : 0.1120694950222969 \n",
      "epoch [118/200] | batch [200/313] | loss : 0.1240057498216629 \n",
      "epoch [118/200] | batch [300/313] | loss : 0.1308145821094513 \n",
      "epoch 118/200 avg train loss: 0.10614924423229961 \n",
      "epoch 118/200 avg val loss: 0.4872526462319531 \n",
      "epoch [119/200] | batch [100/313] | loss : 0.11328441649675369 \n",
      "epoch [119/200] | batch [200/313] | loss : 0.10802766680717468 \n",
      "epoch [119/200] | batch [300/313] | loss : 0.09640001505613327 \n",
      "epoch 119/200 avg train loss: 0.11085694085675687 \n",
      "epoch 119/200 avg val loss: 0.47383355170111113 \n",
      "epoch [120/200] | batch [100/313] | loss : 0.07722011208534241 \n",
      "epoch [120/200] | batch [200/313] | loss : 0.17443326115608215 \n",
      "epoch [120/200] | batch [300/313] | loss : 0.07890737801790237 \n",
      "epoch 120/200 avg train loss: 0.10431075349640541 \n",
      "epoch 120/200 avg val loss: 0.49911555723298956 \n",
      "epoch [121/200] | batch [100/313] | loss : 0.16503357887268066 \n",
      "epoch [121/200] | batch [200/313] | loss : 0.09917978942394257 \n",
      "epoch [121/200] | batch [300/313] | loss : 0.12650902569293976 \n",
      "epoch 121/200 avg train loss: 0.10033286027253245 \n",
      "epoch 121/200 avg val loss: 0.5030154004881654 \n",
      "epoch [122/200] | batch [100/313] | loss : 0.12405072152614594 \n",
      "epoch [122/200] | batch [200/313] | loss : 0.08621785789728165 \n",
      "epoch [122/200] | batch [300/313] | loss : 0.09088058024644852 \n",
      "epoch 122/200 avg train loss: 0.10526033057072483 \n",
      "epoch 122/200 avg val loss: 0.49241858833952795 \n",
      "epoch [123/200] | batch [100/313] | loss : 0.13504083454608917 \n",
      "epoch [123/200] | batch [200/313] | loss : 0.08305356651544571 \n",
      "epoch [123/200] | batch [300/313] | loss : 0.1414036601781845 \n",
      "epoch 123/200 avg train loss: 0.09965890947098549 \n",
      "epoch 123/200 avg val loss: 0.4655185447463506 \n",
      "epoch [124/200] | batch [100/313] | loss : 0.07728610932826996 \n",
      "epoch [124/200] | batch [200/313] | loss : 0.19016984105110168 \n",
      "epoch [124/200] | batch [300/313] | loss : 0.07064258307218552 \n",
      "epoch 124/200 avg train loss: 0.1023235390313898 \n",
      "epoch 124/200 avg val loss: 0.5310129940132552 \n",
      "epoch [125/200] | batch [100/313] | loss : 0.04869796335697174 \n",
      "epoch [125/200] | batch [200/313] | loss : 0.11288557946681976 \n",
      "epoch [125/200] | batch [300/313] | loss : 0.12301040440797806 \n",
      "epoch 125/200 avg train loss: 0.10241625894801304 \n",
      "epoch 125/200 avg val loss: 0.49409823327124874 \n",
      "epoch [126/200] | batch [100/313] | loss : 0.11348477751016617 \n",
      "epoch [126/200] | batch [200/313] | loss : 0.1018461063504219 \n",
      "epoch [126/200] | batch [300/313] | loss : 0.08536044508218765 \n",
      "epoch 126/200 avg train loss: 0.10177899634066861 \n",
      "epoch 126/200 avg val loss: 0.5173375870608077 \n",
      "epoch [127/200] | batch [100/313] | loss : 0.08347799628973007 \n",
      "epoch [127/200] | batch [200/313] | loss : 0.10187267512083054 \n",
      "epoch [127/200] | batch [300/313] | loss : 0.1351049542427063 \n",
      "epoch 127/200 avg train loss: 0.096562656547172 \n",
      "epoch 127/200 avg val loss: 0.4849367520854443 \n",
      "epoch [128/200] | batch [100/313] | loss : 0.09056935459375381 \n",
      "epoch [128/200] | batch [200/313] | loss : 0.09462910890579224 \n",
      "epoch [128/200] | batch [300/313] | loss : 0.10769393295049667 \n",
      "epoch 128/200 avg train loss: 0.09953021405699154 \n",
      "epoch 128/200 avg val loss: 0.5221829144637796 \n",
      "epoch [129/200] | batch [100/313] | loss : 0.06352537125349045 \n",
      "epoch [129/200] | batch [200/313] | loss : 0.09439995884895325 \n",
      "epoch [129/200] | batch [300/313] | loss : 0.07581007480621338 \n",
      "epoch 129/200 avg train loss: 0.0982839227996219 \n",
      "epoch 129/200 avg val loss: 0.5109404819675639 \n",
      "epoch [130/200] | batch [100/313] | loss : 0.0805712416768074 \n",
      "epoch [130/200] | batch [200/313] | loss : 0.07643552869558334 \n",
      "epoch [130/200] | batch [300/313] | loss : 0.1288282871246338 \n",
      "epoch 130/200 avg train loss: 0.0944645372978129 \n",
      "epoch 130/200 avg val loss: 0.4796575136199782 \n",
      "epoch [131/200] | batch [100/313] | loss : 0.06181030347943306 \n",
      "epoch [131/200] | batch [200/313] | loss : 0.08718664944171906 \n",
      "epoch [131/200] | batch [300/313] | loss : 0.1011243462562561 \n",
      "epoch 131/200 avg train loss: 0.09540636389971541 \n",
      "epoch 131/200 avg val loss: 0.5282602147965492 \n",
      "epoch [132/200] | batch [100/313] | loss : 0.13311652839183807 \n",
      "epoch [132/200] | batch [200/313] | loss : 0.17168469727039337 \n",
      "epoch [132/200] | batch [300/313] | loss : 0.14383305609226227 \n",
      "epoch 132/200 avg train loss: 0.09532848784860712 \n",
      "epoch 132/200 avg val loss: 0.5025971037677571 \n",
      "epoch [133/200] | batch [100/313] | loss : 0.11599744111299515 \n",
      "epoch [133/200] | batch [200/313] | loss : 0.057799454778432846 \n",
      "epoch [133/200] | batch [300/313] | loss : 0.07807103544473648 \n",
      "epoch 133/200 avg train loss: 0.09255807858686478 \n",
      "epoch 133/200 avg val loss: 0.49130554157721845 \n",
      "epoch [134/200] | batch [100/313] | loss : 0.08895957469940186 \n",
      "epoch [134/200] | batch [200/313] | loss : 0.08011547476053238 \n",
      "epoch [134/200] | batch [300/313] | loss : 0.12664957344532013 \n",
      "epoch 134/200 avg train loss: 0.08986338857597055 \n",
      "epoch 134/200 avg val loss: 0.5509573896474476 \n",
      "epoch [135/200] | batch [100/313] | loss : 0.10655990242958069 \n",
      "epoch [135/200] | batch [200/313] | loss : 0.08064918965101242 \n",
      "epoch [135/200] | batch [300/313] | loss : 0.10579032450914383 \n",
      "epoch 135/200 avg train loss: 0.0946000223866286 \n",
      "epoch 135/200 avg val loss: 0.5024967280369771 \n",
      "epoch [136/200] | batch [100/313] | loss : 0.09418857842683792 \n",
      "epoch [136/200] | batch [200/313] | loss : 0.06563441455364227 \n",
      "epoch [136/200] | batch [300/313] | loss : 0.08133730292320251 \n",
      "epoch 136/200 avg train loss: 0.08897619130321965 \n",
      "epoch 136/200 avg val loss: 0.5126588193676139 \n",
      "epoch [137/200] | batch [100/313] | loss : 0.08110827952623367 \n",
      "epoch [137/200] | batch [200/313] | loss : 0.03909969702363014 \n",
      "epoch [137/200] | batch [300/313] | loss : 0.08124980330467224 \n",
      "epoch 137/200 avg train loss: 0.08845850756873909 \n",
      "epoch 137/200 avg val loss: 0.5080950944861279 \n",
      "epoch [138/200] | batch [100/313] | loss : 0.047180477529764175 \n",
      "epoch [138/200] | batch [200/313] | loss : 0.10601182281970978 \n",
      "epoch [138/200] | batch [300/313] | loss : 0.06918496638536453 \n",
      "epoch 138/200 avg train loss: 0.09100412101315233 \n",
      "epoch 138/200 avg val loss: 0.5269882563925996 \n",
      "epoch [139/200] | batch [100/313] | loss : 0.0897243544459343 \n",
      "epoch [139/200] | batch [200/313] | loss : 0.04851059243083 \n",
      "epoch [139/200] | batch [300/313] | loss : 0.06537070870399475 \n",
      "epoch 139/200 avg train loss: 0.08998175082020105 \n",
      "epoch 139/200 avg val loss: 0.5017955778520319 \n",
      "epoch [140/200] | batch [100/313] | loss : 0.08139059692621231 \n",
      "epoch [140/200] | batch [200/313] | loss : 0.042512260377407074 \n",
      "epoch [140/200] | batch [300/313] | loss : 0.12249236553907394 \n",
      "epoch 140/200 avg train loss: 0.08506025060916099 \n",
      "epoch 140/200 avg val loss: 0.5549965500831604 \n",
      "epoch [141/200] | batch [100/313] | loss : 0.03654850646853447 \n",
      "epoch [141/200] | batch [200/313] | loss : 0.046865612268447876 \n",
      "epoch [141/200] | batch [300/313] | loss : 0.1408437043428421 \n",
      "epoch 141/200 avg train loss: 0.08965109485073593 \n",
      "epoch 141/200 avg val loss: 0.5651978927699826 \n",
      "epoch [142/200] | batch [100/313] | loss : 0.07398781925439835 \n",
      "epoch [142/200] | batch [200/313] | loss : 0.02639821730554104 \n",
      "epoch [142/200] | batch [300/313] | loss : 0.08194021135568619 \n",
      "epoch 142/200 avg train loss: 0.08648134975697096 \n",
      "epoch 142/200 avg val loss: 0.5265861161147491 \n",
      "epoch [143/200] | batch [100/313] | loss : 0.05253595486283302 \n",
      "epoch [143/200] | batch [200/313] | loss : 0.07269924134016037 \n",
      "epoch [143/200] | batch [300/313] | loss : 0.052657388150691986 \n",
      "epoch 143/200 avg train loss: 0.087012809847062 \n",
      "epoch 143/200 avg val loss: 0.5209306501135041 \n",
      "epoch [144/200] | batch [100/313] | loss : 0.07790900021791458 \n",
      "epoch [144/200] | batch [200/313] | loss : 0.09185478836297989 \n",
      "epoch [144/200] | batch [300/313] | loss : 0.06604516506195068 \n",
      "epoch 144/200 avg train loss: 0.08523923436554666 \n",
      "epoch 144/200 avg val loss: 0.5195401797566233 \n",
      "epoch [145/200] | batch [100/313] | loss : 0.05533699318766594 \n",
      "epoch [145/200] | batch [200/313] | loss : 0.1192406415939331 \n",
      "epoch [145/200] | batch [300/313] | loss : 0.12985797226428986 \n",
      "epoch 145/200 avg train loss: 0.08397262126278764 \n",
      "epoch 145/200 avg val loss: 0.5601703622673131 \n",
      "epoch [146/200] | batch [100/313] | loss : 0.08858612179756165 \n",
      "epoch [146/200] | batch [200/313] | loss : 0.09612420201301575 \n",
      "epoch [146/200] | batch [300/313] | loss : 0.14734870195388794 \n",
      "epoch 146/200 avg train loss: 0.08628139074760885 \n",
      "epoch 146/200 avg val loss: 0.5449900223484522 \n",
      "epoch [147/200] | batch [100/313] | loss : 0.02878633514046669 \n",
      "epoch [147/200] | batch [200/313] | loss : 0.0927530974149704 \n",
      "epoch [147/200] | batch [300/313] | loss : 0.06925709545612335 \n",
      "epoch 147/200 avg train loss: 0.08597964561356904 \n",
      "epoch 147/200 avg val loss: 0.5067767038752761 \n",
      "epoch [148/200] | batch [100/313] | loss : 0.04775901883840561 \n",
      "epoch [148/200] | batch [200/313] | loss : 0.024820322170853615 \n",
      "epoch [148/200] | batch [300/313] | loss : 0.06495988368988037 \n",
      "epoch 148/200 avg train loss: 0.08183672462408535 \n",
      "epoch 148/200 avg val loss: 0.504569690061521 \n",
      "epoch [149/200] | batch [100/313] | loss : 0.05222373828291893 \n",
      "epoch [149/200] | batch [200/313] | loss : 0.06399448961019516 \n",
      "epoch [149/200] | batch [300/313] | loss : 0.0882665365934372 \n",
      "epoch 149/200 avg train loss: 0.08122147326342785 \n",
      "epoch 149/200 avg val loss: 0.4970603492818301 \n",
      "epoch [150/200] | batch [100/313] | loss : 0.10894358158111572 \n",
      "epoch [150/200] | batch [200/313] | loss : 0.1481187343597412 \n",
      "epoch [150/200] | batch [300/313] | loss : 0.06136379390954971 \n",
      "epoch 150/200 avg train loss: 0.08535211633581419 \n",
      "epoch 150/200 avg val loss: 0.5309813294229628 \n",
      "epoch [151/200] | batch [100/313] | loss : 0.08685074746608734 \n",
      "epoch [151/200] | batch [200/313] | loss : 0.056095801293849945 \n",
      "epoch [151/200] | batch [300/313] | loss : 0.10251858830451965 \n",
      "epoch 151/200 avg train loss: 0.07922883949185998 \n",
      "epoch 151/200 avg val loss: 0.5333381336701067 \n",
      "epoch [152/200] | batch [100/313] | loss : 0.07403849810361862 \n",
      "epoch [152/200] | batch [200/313] | loss : 0.03609643504023552 \n",
      "epoch [152/200] | batch [300/313] | loss : 0.11598898470401764 \n",
      "epoch 152/200 avg train loss: 0.08342611330885666 \n",
      "epoch 152/200 avg val loss: 0.5152973472317562 \n",
      "epoch [153/200] | batch [100/313] | loss : 0.05664435029029846 \n",
      "epoch [153/200] | batch [200/313] | loss : 0.08054132014513016 \n",
      "epoch [153/200] | batch [300/313] | loss : 0.07501037418842316 \n",
      "epoch 153/200 avg train loss: 0.07568177833748511 \n",
      "epoch 153/200 avg val loss: 0.5165497307913213 \n",
      "epoch [154/200] | batch [100/313] | loss : 0.13135209679603577 \n",
      "epoch [154/200] | batch [200/313] | loss : 0.16283369064331055 \n",
      "epoch [154/200] | batch [300/313] | loss : 0.06745254993438721 \n",
      "epoch 154/200 avg train loss: 0.07502588653526367 \n",
      "epoch 154/200 avg val loss: 0.5395402759313583 \n",
      "epoch [155/200] | batch [100/313] | loss : 0.050601378083229065 \n",
      "epoch [155/200] | batch [200/313] | loss : 0.04355292394757271 \n",
      "epoch [155/200] | batch [300/313] | loss : 0.059826113283634186 \n",
      "epoch 155/200 avg train loss: 0.07614037438858146 \n",
      "epoch 155/200 avg val loss: 0.5099947429910491 \n",
      "epoch [156/200] | batch [100/313] | loss : 0.17220763862133026 \n",
      "epoch [156/200] | batch [200/313] | loss : 0.06444650143384933 \n",
      "epoch [156/200] | batch [300/313] | loss : 0.11295880377292633 \n",
      "epoch 156/200 avg train loss: 0.07445006668805695 \n",
      "epoch 156/200 avg val loss: 0.4867149059531055 \n",
      "epoch [157/200] | batch [100/313] | loss : 0.08663291484117508 \n",
      "epoch [157/200] | batch [200/313] | loss : 0.07920051366090775 \n",
      "epoch [157/200] | batch [300/313] | loss : 0.12318418174982071 \n",
      "epoch 157/200 avg train loss: 0.07840087607550544 \n",
      "epoch 157/200 avg val loss: 0.5719838074490994 \n",
      "epoch [158/200] | batch [100/313] | loss : 0.04511004686355591 \n",
      "epoch [158/200] | batch [200/313] | loss : 0.07658218592405319 \n",
      "epoch [158/200] | batch [300/313] | loss : 0.11001785099506378 \n",
      "epoch 158/200 avg train loss: 0.07549399050018087 \n",
      "epoch 158/200 avg val loss: 0.5186188122894191 \n",
      "epoch [159/200] | batch [100/313] | loss : 0.06521304696798325 \n",
      "epoch [159/200] | batch [200/313] | loss : 0.07499339431524277 \n",
      "epoch [159/200] | batch [300/313] | loss : 0.06360562145709991 \n",
      "epoch 159/200 avg train loss: 0.07148465626953414 \n",
      "epoch 159/200 avg val loss: 0.5153750607484504 \n",
      "epoch [160/200] | batch [100/313] | loss : 0.16315993666648865 \n",
      "epoch [160/200] | batch [200/313] | loss : 0.07152871787548065 \n",
      "epoch [160/200] | batch [300/313] | loss : 0.08902304619550705 \n",
      "epoch 160/200 avg train loss: 0.07486789063595164 \n",
      "epoch 160/200 avg val loss: 0.5226259701236894 \n",
      "epoch [161/200] | batch [100/313] | loss : 0.07982020825147629 \n",
      "epoch [161/200] | batch [200/313] | loss : 0.06653492152690887 \n",
      "epoch [161/200] | batch [300/313] | loss : 0.09834861755371094 \n",
      "epoch 161/200 avg train loss: 0.07678793097575443 \n",
      "epoch 161/200 avg val loss: 0.5218562438518186 \n",
      "epoch [162/200] | batch [100/313] | loss : 0.04941549524664879 \n",
      "epoch [162/200] | batch [200/313] | loss : 0.06412610411643982 \n",
      "epoch [162/200] | batch [300/313] | loss : 0.08995278924703598 \n",
      "epoch 162/200 avg train loss: 0.07287418125227046 \n",
      "epoch 162/200 avg val loss: 0.5218944374141814 \n",
      "epoch [163/200] | batch [100/313] | loss : 0.06882981210947037 \n",
      "epoch [163/200] | batch [200/313] | loss : 0.120692677795887 \n",
      "epoch [163/200] | batch [300/313] | loss : 0.09636557102203369 \n",
      "epoch 163/200 avg train loss: 0.0734860024001366 \n",
      "epoch 163/200 avg val loss: 0.5422185269903533 \n",
      "epoch [164/200] | batch [100/313] | loss : 0.07315120100975037 \n",
      "epoch [164/200] | batch [200/313] | loss : 0.12701529264450073 \n",
      "epoch [164/200] | batch [300/313] | loss : 0.10530496388673782 \n",
      "epoch 164/200 avg train loss: 0.07503694818375971 \n",
      "epoch 164/200 avg val loss: 0.5122438403247278 \n",
      "epoch [165/200] | batch [100/313] | loss : 0.07074778527021408 \n",
      "epoch [165/200] | batch [200/313] | loss : 0.0681208148598671 \n",
      "epoch [165/200] | batch [300/313] | loss : 0.06952179968357086 \n",
      "epoch 165/200 avg train loss: 0.07305356738464044 \n",
      "epoch 165/200 avg val loss: 0.5260870046206291 \n",
      "epoch [166/200] | batch [100/313] | loss : 0.04419492185115814 \n",
      "epoch [166/200] | batch [200/313] | loss : 0.11602783203125 \n",
      "epoch [166/200] | batch [300/313] | loss : 0.08116339892148972 \n",
      "epoch 166/200 avg train loss: 0.07127778396343652 \n",
      "epoch 166/200 avg val loss: 0.5135635175282443 \n",
      "epoch [167/200] | batch [100/313] | loss : 0.07472126185894012 \n",
      "epoch [167/200] | batch [200/313] | loss : 0.049692463129758835 \n",
      "epoch [167/200] | batch [300/313] | loss : 0.07740022242069244 \n",
      "epoch 167/200 avg train loss: 0.07466907553469983 \n",
      "epoch 167/200 avg val loss: 0.5225652049802527 \n",
      "epoch [168/200] | batch [100/313] | loss : 0.09712357819080353 \n",
      "epoch [168/200] | batch [200/313] | loss : 0.06098949909210205 \n",
      "epoch [168/200] | batch [300/313] | loss : 0.06756230443716049 \n",
      "epoch 168/200 avg train loss: 0.06953091009820517 \n",
      "epoch 168/200 avg val loss: 0.5073064735418633 \n",
      "epoch [169/200] | batch [100/313] | loss : 0.08676765859127045 \n",
      "epoch [169/200] | batch [200/313] | loss : 0.0416281595826149 \n",
      "epoch [169/200] | batch [300/313] | loss : 0.038349226117134094 \n",
      "epoch 169/200 avg train loss: 0.06671252053457137 \n",
      "epoch 169/200 avg val loss: 0.5066141470135013 \n",
      "epoch [170/200] | batch [100/313] | loss : 0.06026767939329147 \n",
      "epoch [170/200] | batch [200/313] | loss : 0.10391855239868164 \n",
      "epoch [170/200] | batch [300/313] | loss : 0.12561815977096558 \n",
      "epoch 170/200 avg train loss: 0.07052499456955982 \n",
      "epoch 170/200 avg val loss: 0.564113085405736 \n",
      "epoch [171/200] | batch [100/313] | loss : 0.06906966120004654 \n",
      "epoch [171/200] | batch [200/313] | loss : 0.06055527925491333 \n",
      "epoch [171/200] | batch [300/313] | loss : 0.05611083656549454 \n",
      "epoch 171/200 avg train loss: 0.07105742524059626 \n",
      "epoch 171/200 avg val loss: 0.5502512387082547 \n",
      "epoch [172/200] | batch [100/313] | loss : 0.05514245480298996 \n",
      "epoch [172/200] | batch [200/313] | loss : 0.057121507823467255 \n",
      "epoch [172/200] | batch [300/313] | loss : 0.07936787605285645 \n",
      "epoch 172/200 avg train loss: 0.06853314395101306 \n",
      "epoch 172/200 avg val loss: 0.5274966274258457 \n",
      "epoch [173/200] | batch [100/313] | loss : 0.06101490184664726 \n",
      "epoch [173/200] | batch [200/313] | loss : 0.02940371260046959 \n",
      "epoch [173/200] | batch [300/313] | loss : 0.055176522582769394 \n",
      "epoch 173/200 avg train loss: 0.06861085371789746 \n",
      "epoch 173/200 avg val loss: 0.5551839768886566 \n",
      "epoch [174/200] | batch [100/313] | loss : 0.06236493960022926 \n",
      "epoch [174/200] | batch [200/313] | loss : 0.10670015215873718 \n",
      "epoch [174/200] | batch [300/313] | loss : 0.0575241781771183 \n",
      "epoch 174/200 avg train loss: 0.06836892536487252 \n",
      "epoch 174/200 avg val loss: 0.5436357010768939 \n",
      "epoch [175/200] | batch [100/313] | loss : 0.0594390407204628 \n",
      "epoch [175/200] | batch [200/313] | loss : 0.11002617329359055 \n",
      "epoch [175/200] | batch [300/313] | loss : 0.08206208795309067 \n",
      "epoch 175/200 avg train loss: 0.07112023637543757 \n",
      "epoch 175/200 avg val loss: 0.5312969320559804 \n",
      "epoch [176/200] | batch [100/313] | loss : 0.10925260186195374 \n",
      "epoch [176/200] | batch [200/313] | loss : 0.09212201088666916 \n",
      "epoch [176/200] | batch [300/313] | loss : 0.07258272916078568 \n",
      "epoch 176/200 avg train loss: 0.06877771936678373 \n",
      "epoch 176/200 avg val loss: 0.513293970612031 \n",
      "epoch [177/200] | batch [100/313] | loss : 0.11388535797595978 \n",
      "epoch [177/200] | batch [200/313] | loss : 0.0932411327958107 \n",
      "epoch [177/200] | batch [300/313] | loss : 0.11255121976137161 \n",
      "epoch 177/200 avg train loss: 0.0691324733733274 \n",
      "epoch 177/200 avg val loss: 0.5321601172791252 \n",
      "epoch [178/200] | batch [100/313] | loss : 0.08605757355690002 \n",
      "epoch [178/200] | batch [200/313] | loss : 0.044057417660951614 \n",
      "epoch [178/200] | batch [300/313] | loss : 0.034679681062698364 \n",
      "epoch 178/200 avg train loss: 0.06296841347048553 \n",
      "epoch 178/200 avg val loss: 0.5248526214044306 \n",
      "epoch [179/200] | batch [100/313] | loss : 0.0726160854101181 \n",
      "epoch [179/200] | batch [200/313] | loss : 0.04176964983344078 \n",
      "epoch [179/200] | batch [300/313] | loss : 0.09074123948812485 \n",
      "epoch 179/200 avg train loss: 0.06346799363605321 \n",
      "epoch 179/200 avg val loss: 0.5091896106170702 \n",
      "epoch [180/200] | batch [100/313] | loss : 0.05950542911887169 \n",
      "epoch [180/200] | batch [200/313] | loss : 0.028722183778882027 \n",
      "epoch [180/200] | batch [300/313] | loss : 0.040464479476213455 \n",
      "epoch 180/200 avg train loss: 0.06249378074984105 \n",
      "epoch 180/200 avg val loss: 0.49638326258599 \n",
      "epoch [181/200] | batch [100/313] | loss : 0.08928335458040237 \n",
      "epoch [181/200] | batch [200/313] | loss : 0.043246619403362274 \n",
      "epoch [181/200] | batch [300/313] | loss : 0.04484918713569641 \n",
      "epoch 181/200 avg train loss: 0.06348903549793429 \n",
      "epoch 181/200 avg val loss: 0.5361176268963874 \n",
      "epoch [182/200] | batch [100/313] | loss : 0.028606250882148743 \n",
      "epoch [182/200] | batch [200/313] | loss : 0.03138326480984688 \n",
      "epoch [182/200] | batch [300/313] | loss : 0.036679111421108246 \n",
      "epoch 182/200 avg train loss: 0.06393553450489387 \n",
      "epoch 182/200 avg val loss: 0.5530926279629333 \n",
      "epoch [183/200] | batch [100/313] | loss : 0.018149450421333313 \n",
      "epoch [183/200] | batch [200/313] | loss : 0.06174426153302193 \n",
      "epoch [183/200] | batch [300/313] | loss : 0.020088357850909233 \n",
      "epoch 183/200 avg train loss: 0.0646565597438917 \n",
      "epoch 183/200 avg val loss: 0.5302525920015347 \n",
      "epoch [184/200] | batch [100/313] | loss : 0.10050743073225021 \n",
      "epoch [184/200] | batch [200/313] | loss : 0.09428676217794418 \n",
      "epoch [184/200] | batch [300/313] | loss : 0.09284185618162155 \n",
      "epoch 184/200 avg train loss: 0.06412990754261946 \n",
      "epoch 184/200 avg val loss: 0.5226252614697323 \n",
      "epoch [185/200] | batch [100/313] | loss : 0.030559184029698372 \n",
      "epoch [185/200] | batch [200/313] | loss : 0.04249734804034233 \n",
      "epoch [185/200] | batch [300/313] | loss : 0.05712973698973656 \n",
      "epoch 185/200 avg train loss: 0.060984155612465105 \n",
      "epoch 185/200 avg val loss: 0.5192229587443268 \n",
      "epoch [186/200] | batch [100/313] | loss : 0.041990604251623154 \n",
      "epoch [186/200] | batch [200/313] | loss : 0.03041297383606434 \n",
      "epoch [186/200] | batch [300/313] | loss : 0.046156395226716995 \n",
      "epoch 186/200 avg train loss: 0.05998265745040898 \n",
      "epoch 186/200 avg val loss: 0.5280488035346889 \n",
      "epoch [187/200] | batch [100/313] | loss : 0.034907713532447815 \n",
      "epoch [187/200] | batch [200/313] | loss : 0.03835739567875862 \n",
      "epoch [187/200] | batch [300/313] | loss : 0.023406032472848892 \n",
      "epoch 187/200 avg train loss: 0.06203777841426218 \n",
      "epoch 187/200 avg val loss: 0.560551820676538 \n",
      "epoch [188/200] | batch [100/313] | loss : 0.06594038754701614 \n",
      "epoch [188/200] | batch [200/313] | loss : 0.07003512233495712 \n",
      "epoch [188/200] | batch [300/313] | loss : 0.03213632479310036 \n",
      "epoch 188/200 avg train loss: 0.06452535536771004 \n",
      "epoch 188/200 avg val loss: 0.5337995820784871 \n",
      "epoch [189/200] | batch [100/313] | loss : 0.06876876205205917 \n",
      "epoch [189/200] | batch [200/313] | loss : 0.051480215042829514 \n",
      "epoch [189/200] | batch [300/313] | loss : 0.09074971824884415 \n",
      "epoch 189/200 avg train loss: 0.0581877295808575 \n",
      "epoch 189/200 avg val loss: 0.5295665890355653 \n",
      "epoch [190/200] | batch [100/313] | loss : 0.06632604449987411 \n",
      "epoch [190/200] | batch [200/313] | loss : 0.06208907067775726 \n",
      "epoch [190/200] | batch [300/313] | loss : 0.05221026763319969 \n",
      "epoch 190/200 avg train loss: 0.06068051422830112 \n",
      "epoch 190/200 avg val loss: 0.5570471154738076 \n",
      "epoch [191/200] | batch [100/313] | loss : 0.04133311286568642 \n",
      "epoch [191/200] | batch [200/313] | loss : 0.11152076721191406 \n",
      "epoch [191/200] | batch [300/313] | loss : 0.05405030399560928 \n",
      "epoch 191/200 avg train loss: 0.060574238642324175 \n",
      "epoch 191/200 avg val loss: 0.5382824144031428 \n",
      "epoch [192/200] | batch [100/313] | loss : 0.06141214445233345 \n",
      "epoch [192/200] | batch [200/313] | loss : 0.07366342842578888 \n",
      "epoch [192/200] | batch [300/313] | loss : 0.050487175583839417 \n",
      "epoch 192/200 avg train loss: 0.0596810396248921 \n",
      "epoch 192/200 avg val loss: 0.5486485003670559 \n",
      "epoch [193/200] | batch [100/313] | loss : 0.09729809314012527 \n",
      "epoch [193/200] | batch [200/313] | loss : 0.02542247623205185 \n",
      "epoch [193/200] | batch [300/313] | loss : 0.041152819991111755 \n",
      "epoch 193/200 avg train loss: 0.06537083949233158 \n",
      "epoch 193/200 avg val loss: 0.5266714920348758 \n",
      "epoch [194/200] | batch [100/313] | loss : 0.046868354082107544 \n",
      "epoch [194/200] | batch [200/313] | loss : 0.061983101069927216 \n",
      "epoch [194/200] | batch [300/313] | loss : 0.038594841957092285 \n",
      "epoch 194/200 avg train loss: 0.05976610238583514 \n",
      "epoch 194/200 avg val loss: 0.5209349880490122 \n",
      "epoch [195/200] | batch [100/313] | loss : 0.042144838720560074 \n",
      "epoch [195/200] | batch [200/313] | loss : 0.05866972729563713 \n",
      "epoch [195/200] | batch [300/313] | loss : 0.04144485667347908 \n",
      "epoch 195/200 avg train loss: 0.056660028582159135 \n",
      "epoch 195/200 avg val loss: 0.5426826995762088 \n",
      "epoch [196/200] | batch [100/313] | loss : 0.02134585566818714 \n",
      "epoch [196/200] | batch [200/313] | loss : 0.11550650745630264 \n",
      "epoch [196/200] | batch [300/313] | loss : 0.04786565899848938 \n",
      "epoch 196/200 avg train loss: 0.058149789897397684 \n",
      "epoch 196/200 avg val loss: 0.5369887646240524 \n",
      "epoch [197/200] | batch [100/313] | loss : 0.0319552905857563 \n",
      "epoch [197/200] | batch [200/313] | loss : 0.04339373856782913 \n",
      "epoch [197/200] | batch [300/313] | loss : 0.05224663391709328 \n",
      "epoch 197/200 avg train loss: 0.05730415720302171 \n",
      "epoch 197/200 avg val loss: 0.5492908342069462 \n",
      "epoch [198/200] | batch [100/313] | loss : 0.04146667569875717 \n",
      "epoch [198/200] | batch [200/313] | loss : 0.04440977796912193 \n",
      "epoch [198/200] | batch [300/313] | loss : 0.07961233705282211 \n",
      "epoch 198/200 avg train loss: 0.058104182391192395 \n",
      "epoch 198/200 avg val loss: 0.5091115453197986 \n",
      "epoch [199/200] | batch [100/313] | loss : 0.04416738823056221 \n",
      "epoch [199/200] | batch [200/313] | loss : 0.041348762810230255 \n",
      "epoch [199/200] | batch [300/313] | loss : 0.0691266879439354 \n",
      "epoch 199/200 avg train loss: 0.05629395502812851 \n",
      "epoch 199/200 avg val loss: 0.5358384200666524 \n",
      "epoch [200/200] | batch [100/313] | loss : 0.021313153207302094 \n",
      "epoch [200/200] | batch [200/313] | loss : 0.0715145617723465 \n",
      "epoch [200/200] | batch [300/313] | loss : 0.03867959603667259 \n",
      "epoch 200/200 avg train loss: 0.054940208539771385 \n",
      "epoch 200/200 avg val loss: 0.516071688902529 \n"
     ]
    }
   ],
   "source": [
    "model= ObjectClassifier().to(device)\n",
    "\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer= torch.optim.AdamW(model.parameters(), lr= 3e-4, weight_decay=1e-3)\n",
    "scheduler= torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=200, eta_min=1e-6)\n",
    "\n",
    "num_epochs=200\n",
    "\n",
    "best_loss= float(\"inf\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    running_loss=0\n",
    "\n",
    "    for batch_idx, (images,labels) in enumerate(train_loader):\n",
    "\n",
    "        images= images.to(device)\n",
    "        labels= labels.to(device)\n",
    "\n",
    "        output= model(images)\n",
    "        loss= criterion(output, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (batch_idx+1)%100==0:\n",
    "\n",
    "            print(f\"epoch [{epoch+1}/{num_epochs}] | batch [{batch_idx+1}/{len(train_loader)}] | loss : {loss.item()} \")\n",
    "\n",
    "    average_train_loss= running_loss/len(train_loader)\n",
    "    print(f\"epoch {epoch+1}/{num_epochs} avg train loss: {average_train_loss} \")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_val_loss=0\n",
    "\n",
    "        for image,label in val_loader:\n",
    "            image= image.to(device)\n",
    "            label= label.to(device)\n",
    "\n",
    "            output= model(image)\n",
    "            loss= criterion(output,label)\n",
    "\n",
    "            running_val_loss+= loss.item()\n",
    "\n",
    "        avg_val_loss= running_val_loss/len(val_loader)\n",
    "        print(f\"epoch {epoch+1}/{num_epochs} avg val loss: {avg_val_loss} \")\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss= avg_val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "228101d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18450/1644180327.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.55\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "total=0\n",
    "correct=0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images,labels in test_loader:\n",
    "        images= images.to(device)\n",
    "        labels= labels.to(device)\n",
    "\n",
    "        output=model(images)\n",
    "        _,predicted= torch.max(output,1)\n",
    "\n",
    "        total+= images.size(0)\n",
    "        correct+= (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy= 100 * correct/total\n",
    "    print(accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
